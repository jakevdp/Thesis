\chapter{Introduction to Karhunen-Lo\`{e}ve Analysis}

%\begin{itemize}
%  \item Introduce the notational formalism
%  \item KL as an eigenvalue problem
%  \item KL as a signal-to-noise ranking
%  \item KL as an optimal low-dimensional representation
%  \item KL on noisy data
%  \item KL with missing information
%\end{itemize}

%Also mention LLE and cite \cite{Vanderplas2009}, which uses a nonlinear
%alternative to PCA for dimensionalty reduction.

\KL\ (KL) analysis is a commonly used statistical tool
in a broad range of astronomical applications, from, e.g.~studies of
correlations in observed properties of galaxy photometry \citep{Efstathiou84}
and galaxy and quasar spectra \citep{Connolly95,Connolly99,Yip04a,Yip04b},
to  analysis of the spatial distribution of galaxies 
\citep{Vogeley96, Matsubara00, Pope04}, to characterization of the 
expected errors in weak lensing surveys \citep{Kilbinger06, Munshi06}.
In this chapter, we will develop the formalism of KL which will form
the basis of the applications in the subsequent chapters.

The KL formalism requires the liberal
employment of algebra with vectors, scalars, matrices, and their
generalizations.
For clarity, we will begin by briefly specifying the notational
conventions used in this chapter and throughout this work.

\section{Notational Conventions}

It is important to clearly distinguish between vectors, matrices, and
scalars in the following formulation.  Vectors will be denoted by
bold lower-case symbols; e.g. $\myvec{x}$.  Matrices will be denoted by
bold upper-case symbols; e.g. $\mymat{X}$.  Scalars will be denoted by
non-bold symbols, either upper or lower-case.
All vectors are assumed to be column vectors, while a row-vector is
indicated by the transpose, $\myvec{x}^T$.  
Single elements of a given
vector or matrix are given with subscripts: $x_i$ is the $i^{\rm th}$
element of the vector $\myvec{x}$, and $X_{ij}$ is the element in the
$i^{\rm th}$ row and $j^{\rm th}$ column of the matrix $\mymat{X}$.
The column vector making up the $j^{\rm th}$ row of $\mymat{X}$ is
indicated by $\myvec{x}^{(j)}$.  Note then, that by this convention,
the $(i, j)$ component of matrix $\mymat{X}$ can be equivalently expressed
$X_{ij}$ or $x_i^{(j)}$.

In algebraic expressions, the normal linear algebra rules are assumed.
For example, the expression 
\begin{equation}
  \myvec{y} = \mymat{M}\mymat{x} + \mymat{b}
\end{equation}
involves the vectors $\myvec{y}$, $\myvec{x}$, and $\myvec{b}$ and the
matrix $\mymat{M}$.  This expression is short-hand for the summation:
\begin{equation}
  y_i = \sum_{j=1}^{n} M_{ij} x_j + b_i
\end{equation}
Using these rules, we can define the magnitude of a vector
\begin{equation}
  |\myvec{x}| \equiv \sqrt{\myvec{x}^T \myvec{x}}
\end{equation}

\section{Basis function decomposition}
KL analysis is simply a basis function decomposition, where the basis
functions are derived based on the variance and covariance properties
of a class of functions.  We'll start by describing what is perhaps the
best-known basis function decomposition, the Fourier Series.
We start here because it's a familiar concept which generalizes well to
the fundamental ideas of KL analysis.

\subsection{Fourier Series}
The Fourier Series is a means of expressing a bounded function in terms
of a certain class of oscillatory basis functions.  It is a discrete version
of the Fourier Transforms used in cosmological power spectrum analysis, and
discussed in section XXX.

We'll define a set of oscillatory basis functions 
\begin{equation}
  \label{eq:fourier_basis}
  \Phi_k(t) = \frac{1}{\sqrt{t_b - t_a}}
  \exp\left[\frac{2\pi i k (t-t_a)}{t_b - t_a}\right]
\end{equation}
where $t$ is the arbitrary dependent variable, $k$ is the wave-number,
and the function is defined in the region $t_a \le t \le t_b$.
and postulate that a function $f(t)$ can be expressed as a linear combination
of these basis functions:
\begin{equation}
  \label{eq:fourier_1D}
  f(t) = \sum_{k=-\infty}^\infty a_k\Phi_k(t).
\end{equation}
Here $a_k$ are an infinite set of complex coefficients.  Our claim is that
any piecewise continuous and square-integrable function $f(t)$ in the
interval $[t_a, t_b]$ can be represented this way.
A rigorous mathematical proof
of this statement can be found elsewhere, but below we will lend support
to this claim.

Given the claim that Equation~\ref{eq:fourier_1D} holds, how can we
compute the Fourier coefficients $a_k$ associated with 
a particular $f(t)$?  Though the expression is well-known, we'll briefly
derive it here because it illuminates some of the properties of
Fourier transforms which will generalize to KL transforms.

To begin, we'll multiply both sides of Equation~\ref{eq:fourier_1D} by the
complex conjugate $\Phi^\ast_{k^\prime}(x)$ of the basis function given
in Equation~\ref{eq:fourier_basis}, and integrate both sides over $t$
from $t_a$ to $t_b$:
\begin{equation}
  \int_{t_a}^{t_b} \Phi^\ast_{k^\prime}(t) f(t) \dd t
  = \int_{t_a}^{t_b} \Phi^\ast_{k^\prime}(t) \sum_{k=-\infty}^\infty a_k\Phi_k(x)\dd t
\end{equation}
On the right-hand side, we can exchange the order of integration and
summation to find 
\begin{equation}
  \label{eq:fourier_coef_der}
  \int_{t_a}^{t_b} \Phi^\ast_{k^\prime}(t) f(t) \dd t
  =  \sum_{k=-\infty}^\infty a_k \left[\int_{t_a}^{t_b} \Phi^\ast_{k^\prime}(t)\Phi_k(x)\dd t\right].
\end{equation}
Let's examine the term in the square brackets.  Plugging in the definition
of the basis functions from Equation~\ref{eq:fourier_basis}, we have
\begin{equation}
  \left[\int_{t_a}^{t_b} \Phi^\ast_{k^\prime}(t)\Phi_k(x)\dd t\right]
  = \left[\frac{1}{t_b-t_a}\int_{t_a}^{t_b} \exp\left(\frac{2\pi i (k - k^\prime) (t-t_a)}
    {t_b - t_a}\right)\dd t\right]
\end{equation}
This gives two distinct situations for the integral on the right-hand side:
when $k=k^\prime$, both the integrand 
and the term in the brackets is exactly $1$.  When $k\ne k^\prime$,
the integrand oscillates through an integer number of cycles between
$t_a$ and $t_b$ (remember that $k$ and $k^\prime$ here are integers),
and the result of the integral is exactly $0$.
So we see that the term in brackets is equal to simply 
the Kronecker delta function $\delta_{kk^\prime}$, defined as
\begin{equation}
  \delta_{ij} = \left\{
  \begin{array}{ll}
    1 & {\rm if}\ i = j\\
    0 & {\rm if}\ i \ne j.
  \end{array}
  \right.
\end{equation}
Putting this result into Equation~\ref{eq:fourier_coef_der}, only one
term of the sum remains and we find
\begin{equation}
  \label{eq:fourier_coef}
  a_k = \frac{1}{t_b - t_a}\int_{t_a}^{t_b} \Phi^\ast_k(t) f(t) \dd t
\end{equation}

Equation~\ref{eq:fourier_coef} shows how to compute the Fourier coefficients
$a_k$ for a given $f(t)$.  But one might wonder if this is a unique result.
Could there be several possible sets of valid Fourier coefficients for
a given function?

Let's assume that given a function $f(t)$, there are two valid sets of
Fourier coefficients $a_k$ and $a^\prime_k$ which satisfy
Equation~\ref{eq:fourier_1D}.  In this case, subtracting the two equations
gives
\begin{equation}
  f(t) - f(t) = \sum_{k=-\infty}^\infty (a_k - a^\prime_k)\Phi_k(t) = 0.
\end{equation}
In a similar manner to above, we can multiply by $\Phi^\ast_k(t)$, integrate
over $t$ from $t_a$ to $t_b$, and extract a kronecker delta function to
yield
\begin{equation}
  \sum_{k=-\infty}^\infty (a_k - a^\prime_k) \delta_{kk^\prime} = 0
\end{equation}
Collapsing the sum, we find that $a_k = a^\prime_k$ for all $k$.  This
shows the uniqueness of the fourier coefficients $a_k$ for a given function
$f(t)$ on an interval $[t_a, t_b]$.  Thus, given an orthonormal basis $\Phi_k$,
there is a single unique linear combination which reconstructs a function
$f(t)$ on the defined interval.

\subsection{Generalizing Orthonormal Bases}

Stepping back for a moment, we have shown that for a particular
class of functions $\Phi_k(t)$, one can find unique coefficients
$a_k$ such that one of the expansions of Equation~\ref{eq:fourier_1D} holds.
A key observation is that all the derivations above rested 
soley on two special properties of these basis functions:
\begin{enumerate}
  \item
    The basis functions are {\it orthonormal} on the interval $[a, b]$.
    That is, $\Phi_k(t)$ satisfies
    \begin{equation}
      \int_{t_a}^{t_b} \Phi_k(t) \Phi^\star_{k^\prime}(t) = \delta_{kk^\prime}
    \end{equation}
  \item
    The basis functions are {\it complete} on the interval $[t_a, t_b]$.
    That is, an arbitrary function $f(t)$ can be approximated by
    \begin{equation}
      f(t) = \sum_{k=1}^N a_k \Phi_k(t)
    \end{equation}
    and the mean square error satisfies
    \begin{equation}
      \label{eq:completeness}
      \lim_{N\to\infty} \int_{t_a}^{t_b}
      \left[f(t) - \sum_{k=1}^{N}a_k \Phi_k(t)\right]^2 \dd t = 0.
    \end{equation}
\end{enumerate}
As long as these two properties hold for a class of functions $\Phi_k(t)$,
we would be able to repeat the above derivations and express any $f(t)$
via Equation~\ref{eq:fourier_1D}.
This suggests the possible existence of other functions which fit these
criteria.  Some examples are the Legendre polynomials on the interval
$[-1, 1]$, the Laguerre polynomials on the interval $[0, \infty)$, and
the Hermite polynomials on the interval $(-\infty, \infty)$.
In fact, just as there are an infinite number of possibile orientations for
an $(x, y)$ axis in a two-dimensional vector space, there are an infinite
number of possible orthogonal function bases that work in the above
formalism.  Choosing the right basis can lead to a much easier analysis of
a given problem.

\section{\KL\ Analysis}
Because of the infinite number of possible orthogonal function classes,
one might wonder how to choose the optimal class for any particular problem.
\KL\ Analysis Seeks to answer this question in a very general case.

\subsection{Derivation of \KL\ theorem}
Imagine now that we have a random process $F_t$.  This can be thought of as
an arbitrarily large collection of functions $f^{(i)}(t)$ defined
on the interval
$t \in [a, b]$.  At a given location $t$, the expectation value of the
random process is given by
\begin{equation}
  E[F_t] = \lim_{N \to \infty} \frac{1}{N} \sum_{i=1}^N f^{(i)}(t)
\end{equation}
For simplicity, we'll assume that the random process $F_t$ is {\it
centered}; that is $E[F_t] = 0$.  A general random process can be
centered by subtracting the expectation value for each $t$.
A random process like $F_t$ can be characterized
by its covariance function, which is defined as
\begin{eqnarray}
  \label{eq:corrfunc_def}
  \mathcal{C}_F(t, t^\prime) &\equiv& E[F_t F_{t^\prime}]
  \nonumber\\
  &=& \lim_{N\to\infty}\frac{1}{N} \sum_{i=1}^N
  f^{(i)}(t)f^{(i)}(t^\prime).
\end{eqnarray}
For an {\it uncorrelated} random process,
$\mathcal{C}_F(t, t^\prime) = \mathcal{C}_F(t, t) \delta_{t, t^\prime}$
where $\mathcal{C}_F(t, t) $ is the {\it variance} of $F_t$.

\subsection{Eigenfunctions}
We'll now introduce the {\it eigenfunctions} $e_k(t)$
of the covariance function $\mathcal{C}_F(t, t^\prime)$, which satisfy
\begin{equation}
  \label{eq:eigfunc_def}
  \int_{t_a}^{t_b} \mathcal{C}_F(t, t^\prime) e_k(t^\prime)\dd t^\prime
  = \lambda_k e_k(t)
\end{equation}
subject to the constraint that $e_k(t)$ is not everywhere zero.
Here $\lambda_k$ is the {\it eigenvalue} associated with the
eigenfunction $e_k(t)$.

Now what are the properties of these eigenfunctions?  First of all, they
are orthogonal on the interval $[a, b]$.
We can show this by considering two arbitrary eigenfunctions
$e_k(t)$ and $e_{k^\prime}(t)$.  Consider the quantity
\begin{equation}
  \int_{t_a}^{t_b} \dd t \int_{t_a}^{t_b} \dd t^\prime\,\, \mathcal{C}_F(t, t^\prime)
  e_{k^\prime}(t^\prime) e_k(t)
\end{equation}
Because of the symmetry of the covarience,
i.e.~$\mathcal{C}_F(t, t^\prime) = \mathcal{C}_F(t^\prime, t)$, and
because the order of integration can be switched, this can be evaluated
two different ways, which must be equal:
\begin{eqnarray}
  \int_{t_a}^{t_b} \dd t\,\, e_k(t)
  \int_{t_a}^{t_b} \dd t^\prime\,\, \mathcal{C}_F(t, t^\prime) e_{k^\prime}(t^\prime) &=&
  \int_{t_a}^{t_b} \dd t^\prime\,\, e_k^\prime(t^\prime)
  \int_{t_a}^{t_b} \dd t\,\, \mathcal{C}_F(t^\prime, t) e_{k}(t)
  \nonumber\\
  \int_{t_a}^{t_b} \dd t\,\, \lambda_{k^\prime} e_k(t) e_{k^\prime}(t) &=&
  \int_{t_a}^{t_b} \dd t^\prime\,\, \lambda_k e_k(t^\prime) e_{k^\prime}(t^\prime)
\end{eqnarray}
Rearranging the bottom line leads to
\begin{equation}
  (\lambda_k - \lambda_{k^\prime})
  \int_{t_a}^{t_b} \dd t\,\, e_k(t) e_{k^\prime}(t) = 0
\end{equation}
So for $\lambda_k \ne \lambda_{k^\prime}$, then $e_k$ and $e_{k^\prime}$
must be orthogonal\footnote{In the degenerate case when
$\lambda_k = \lambda_{k^\prime}$, one can still construct orthogonal
vectors by linear combinations:
\begin{eqnarray}
  e_+(t) &=& \frac{e_k(t) + e_{k^\prime}(t)}{\sqrt{2}} \nonumber\\
  e_-(t) &=& \frac{e_k(t) - e_{k^\prime}(t)}{\sqrt{2}}. \nonumber
\end{eqnarray}
This leads to two new orthogonal eigenfunctions $e_+$ and $e_-$ with the
same eigenvalue $\lambda_k$.}.  From the definition in
Equation~\ref{eq:eigfunc_def}, we see that if $e_k(t)$ is an eigenfunction
with eigenvalue $\lambda_k$,
then for any arbitrary constant $K$,
$K\, e_k(t)$ is an eigenfunction with eigenvalue $\lambda_k$ as well.
To make the choice of eigenfunction more definite, we will assume all
eigenfunctions are normalized: that is
\begin{equation}
  \int_{t_a}^{t_b} \dd t e_k(t) e_{k^\prime}(t) = \delta_{kk^\prime}
\end{equation}
for all $k$.  This still allows any eigenfunction to have an arbitrary
phase: that is, an eigenfunction may be multiplied by $e^{i\theta}$ for
any theta, and still satisfy our orthogonality condition.  This fact
will become important later.

The net result is that the eigenfunctions $e_k(t)$ form an orthonormal
basis for the space of functions represented by the random process $F_t$.
In general, the eigenfunctions
also satisfy the completeness relation (see eqn.~\ref{eq:completeness}).
The proof of the completeness of eigenfunctions for a symmetric kernel
$\mathcal{C}_F(t^\prime, t)$ can be found in, e.g.~\citet{Courant1989}.

Let's now consider the expansion of the random process $F_t$ onto the
eigenvectors $e_k(t)$.  Analogously to the Fourier case discussed
above, we have
\begin{equation}
  \label{eq:Ft_decomp}
  F_t = \sum_{k=1}^\infty A_k e_k(t).
\end{equation}
where here $A_k$ can be thought of as a set of coefficients $a_k^{(i)}$
in the same way that the random process $F_t$ can be thought of as a
set of functions $f^{(i)}(t)$.
Multiplying both sides by $e_{k^\prime}(t)$, integrating, and using the
orthogonality of eigenvectors (this is analogous to the derivation
in Equations~\ref{eq:fourier_coef_der}-\ref{eq:fourier_coef}) leads to
\begin{equation}
  \label{eq:F_k_def}
  A_k = \int_{t_a}^{t_b} F_t e_k(t) \dd t
\end{equation}

Because of the fact that the random process is centered (i.e.~$E[F_t] = 0$),
it is straightforward to show that $E[A_k] = 0$ as well.  The more interesting
computation is that of the covariance of the eigenvectors,
$\mathcal{C}_A(k, k^\prime)$.
From Equations~\ref{eq:corrfunc_def} and \ref{eq:F_k_def}, we have
\begin{eqnarray}
  \mathcal{C}_A(k, k^\prime)
  &=& E[A_k A_{k^\prime}]\nonumber\\
  &=& E\left[\int_{t_a}^{t_b} \dd t \int_{t_a}^{t_b} \dd t^\prime
    {F}_t e_k(t)
    {F}_{t^\prime} e_{k^\prime}(t^\prime)\right] \nonumber\\
  &=& \int_{t_a}^{t_b} \dd t \int_{t_a}^{t_b} \dd t^\prime
    E[{F}_t {F}_{t^\prime}]
    e_{k^\prime}(t^\prime) e_k(t) \nonumber\\
  &=& \int_{t_a}^{t_b} \dd t \int_{t_a}^{t_b} \dd t^\prime
    \mathcal{C}_F(t, t^\prime)
    e_{k^\prime}(t^\prime) e_k(t) \nonumber
\end{eqnarray}
Substituting Equation~\ref{eq:eigfunc_def}, we find that this gives
\begin{eqnarray}
  \label{eq:cov_A}
  \mathcal{C}_A(k, k^\prime)
  &=& \int_{t_a}^{t_b} \dd t \lambda_{k^\prime} e_{k^\prime}(t) e_k(t) \nonumber\\
  &=& \lambda_k \delta_{kk^\prime}
\end{eqnarray}
So we see that projection of the centered random process $\tilde{F}_t$ onto
the eigenvectors of its covariance matrix yields coefficients which
are uncorrelated, with variance equal to the eigenvalues $\lambda_k$.
This result is the \KL\ theorem, and it has many ramifications that will
be discussed below.

\subsection{Partial Reconstructions}
\label{sec:partial_recons}
We have shown that \KL\ provides an orthonormal basis for a random field
with uncorrelated projection coefficients.  We can go further and show
that \KL\ provides the
optimal orthonormal basis for low-rank approximations of functions in
a random field.

Above, we expressed the completeness relation for a single function $f(t)$
(eqn.~\ref{eq:completeness}).
For a random process, an orthonormal basis
$\phi_k(t)$ is complete if and only if there
exists a random process $B_k$ such that
\begin{equation}
  \label{eq:completeness_rp}
  \lim_{N\to\infty} \int_{t_a}^{t_b}
  \left[F_t - \sum_{k=1}^{N}B_k \phi_k(t)\right]^2 \dd t = 0.
\end{equation}
A random process is {\it low-rank} if and only if there exists a complete
orthonormal
basis $\phi_k(t)$ such that $E[B_k^2] = 0$ for one or more values of $k$.
In other words, a random process is low-rank if for some basis $\phi_k(t)$,
some values of $k$ are not required for a perfect reconstruction of the
function.

Let us consider an arbitrary complete orthonormal basis
$\phi_k(t)$, with $F_t = \sum_{k=1}^\infty B_k\phi_k(t)$.
Given this basis, we'll define the low-rank approximation of $F_t$
\begin{equation}
  F^{(N)}_t = \sum_{k=1}^N B_k\phi_k(t)
\end{equation}
We'll seek to minimize the expectation value of the squared error
\begin{eqnarray}
  \mathcal{E}^2_N &=& \int_{t_a}^{t_b} \left[F_t - F_t^{(N)}\right]^2 \dd t
  \nonumber\\
  &=& \int_{t_a}^{t_b} \left[\sum_{k = N + 1}^\infty B_k \phi_k(t)\right]^2 \dd t.
\end{eqnarray}


Expanding the sum leads to
\begin{eqnarray}
  \mathcal{E}^2_N &=& \sum_{k=N + 1}^\infty
  \sum_{k^\prime=N + 1}^\infty \int_{t_a}^{t_b} 
  \left[B_k B_{k^\prime} \phi_k(t) \phi_{k^\prime}(t)\right] \dd t
  \nonumber\\
  &=& \sum_{k=N+1}^\infty \sum_{k^\prime=N+1}^\infty B_k B_{k^\prime}
  \delta_{kk^\prime}\nonumber\\
  &=& \sum_{k=N+1}^\infty B_k^2
\end{eqnarray}
Taking the expectation value and plugging in the equivalent of
Equation~\ref{eq:F_k_def} for $B_k$, we find
\begin{eqnarray}
  E[\mathcal{E}^2_N] &=&
  E\left[\sum_{k=N+1}^\infty \int_{t_a}^{t_b} \dd t \int_{t_a}^{t_b} \dd t^\prime
  F_tF_{t^\prime} \phi_m(t)\phi_m(t^\prime)\right] \nonumber\\
  &=& \sum_{k=N+1}^\infty \int_{t_a}^{t_b} \dd t \int_{t_a}^{t_b} \dd t^\prime
  C_F(t, t^\prime) \phi_m(t)\phi_m(t^\prime)
\end{eqnarray}

We'd like to minimize this expected error over the basis $\phi_m(t)$, subject
to the constraint that $\phi_m(t)$ are an orthonormal basis.  We'll accomplish
this by the method of Lagrange multipliers.  Our Lagrangian is
\begin{equation}
  \mathcal{L}(\{\phi(t)\}) = \sum_{k=N+1}^\infty
  \left[\int_{t_a}^{t_b} \dd t \int_{t_a}^{t_b} \dd t^\prime
  C_F(t, t^\prime) \phi_m(t)\phi_m(t^\prime)
  - \lambda_k\left(1 - \int_{t_a}^{t_b} \phi_m(t)^2 \dd t\right)
  \right]
\end{equation}
Minimizing this with respect to $\phi_k$ gives
\begin{equation}
  \frac{\partial\mathcal{L}}{\partial\phi_k(t)} = \int_{t_a}^{t_b} \dd t^\prime
  C_F(t, t^\prime) \phi_k(t^\prime)
  - \lambda_k \phi_k(t).
\end{equation}
The optimumum for each $k$ is where this derivative equals zero; setting to
zero and solving recovers the original eigenvalue problem
 (Eq.~\ref{eq:eigfunc_def}) from which we
derived the KL basis $\{e_k(t)\}$.  By the uniqueness of the eigenvalue
decomposition, this shows that the KL basis is the optimal basis for low-rank
approximations of functions drawn from $F_t$.  Furthermore, for an
approximation using $N$ eigenvectors, the mean squared error is given by
\begin{eqnarray}
  E[\mathcal{E}^2_N] &=& \sum_{k=N+1}^\infty E[A_k^2]\nonumber\\
  &=& \sum_{k=N+1}^\infty \lambda_k
\end{eqnarray}
This is an interesting result: it says that in order to minimize the
expectation value of the reconstruction error $\mathcal{E}_N$ for all $N$,
we simply need to order the eigenvalues such that $\lambda_k \ge \lambda_{k+1}$
for all eigenvalue-eigenfunction pairs $(\lambda_k, e_k(t))$.

Because of this, throughout this work we will follow this convention when
ordering the eigenvalues in and KL decomposition.

\subsection{KL in the presence of noise}
\label{sec:whitening}
In practice, the observed random field $F_t$ is composed of the sum of
a signal $S_t$ and noise $N_t$.  We'll continue to assume that both of these
are centered.  The covariance matrix then becomes
\begin{equation}
  \mathcal{C}_F(t, t^\prime) = E[(S_t + N_t)(S_{t^\prime}+N_{t^\prime})]
\end{equation}
Under the assumption that the signal $S_t$ and noise $N_t$ are uncorrelated,
this can be simplified to
\begin{eqnarray}
  \mathcal{C}_F(t, t^\prime) &=& E[S_t S_{t^\prime}]
  + E[N_t N_{t^\prime}] \nonumber\\
  &=& \mathcal{C}_S(t, t^\prime) + \mathcal{C}_N(t, t^\prime)
\end{eqnarray}
The \KL\ eigenfunctions always diagonalize the full covariance
$\mathcal{C}_F(t, t^\prime)$.
In the case of uncorrelated ``white'' noise,
$\mathcal{C}_N(t, t^\prime) = \sigma^2 \delta(t - t^\prime)$
and both the signal and the noise become diagonalized.  In this case,
the noise per mode is a constant $\sigma^2$, and the ranking of the
eigenfunctions leads to modes which are ranked in signal-to-noise.
This is why KL modes are often referred to as ``signal-to-noise
eigenmodes'' \citep{Vogeley96}.

In cases when the noise is not white, we can still recover signal-to-noise
information by preprocessing with a whitening transformation.  The eigenvectors
$n_k(t)$ of the noise covariance, which satisfy
\begin{equation}
  \int_{t_a}^{t_b} \mathcal{C}_N(t, t^\prime) n_k(t^\prime) \dd t^\prime 
  = \sigma_i n_k(t)
\end{equation}
can be used to apply a whitening transform to both the signal and the
noise.  The whitened covariance is given by
\begin{eqnarray}
  \mathcal{C}_F^{(W)}(k, k^\prime) &\equiv& \int_{t_a}^{t_b}\dd t\int_{t_a}^{t_b}\dd t^\prime
  \mathcal{C}_F(t, t^\prime)
  \frac{n_k(t) n_{k^\prime}(t^\prime)}{\sigma_k\sigma_{k^\prime}}\nonumber\\
  &=& \mathcal{C}_S^{(W)}(k, k^\prime) + \delta_{kk^\prime}.
\end{eqnarray}
The signal is now expressed in the basis of the noise eigenmodes $n_k(t)$,
and the noise has been made white with unit variance.  The KL modes derived
from the whitened covariance $\mathcal{C}_F^{(W)}(k, k^\prime)$ will have
eigenvalues which represent the signal-to-noise in each mode.

\subsection{\KL: theory to practice}
The abstract formalism presented above is interesting in itself, but one
might wonder what practical advantages can be gained from this discussion.
In practice, we don't deal with an abstract stochastic process $F_t$, but
with discrete, measured data.  For this reason, the continuous formalism
from above can be transformed into a discrete linear algebraic formalism,
as seen below.  In this section we will discuss the
practical computational aspects of KL analysis.

Imagine, for the moment, that an astronomer has observed the spectra of
$N$ galaxies.  After normalization and correction for redshift effects,
the spectra can be encoded as a series of $N$ real-valued functions
$f^{(i)}(\lambda)$ over some defined domain $\lambda \in [a, b]$.
In practice, we measure these spectra at a finite set of wavelengths
$\myvec{\lambda}^T = [\lambda_1, \lambda_2 \cdots \lambda_M]$ so that
our observations $\myvec{f}^{(i)}$ become $M$-dimensional vectors.  For
convenience, we'll store these spectra in a $M \times N$ matrix
$\mymat{F} = [\myvec{f}^{(1)}, \myvec{f}^{(2)} \cdots \myvec{f}^{(N)}]^T$,
where each row of the matrix represents one spectrum.
These series of spectra $\mymat{F}$ 
can be considered a finite realization of a particular
random process $F_\lambda$.

The expectation value $E[F_\lambda]$ can be approximated via the sample mean:
\begin{equation}
  E[F_\lambda] \approx \myvec{\bar{f}}
  = \frac{1}{N} \sum_{i=1}^N \myvec{f}^{(i)}
\end{equation}
and the covariance function can be approximated by the covariance matrix
\begin{equation}
  \label{eq:mat_corr_def}
  E[F_\lambda F_{\lambda^\prime}] \approx 
  \mymat{\mathcal{C}_F} = \frac{1}{N - 1} \mymat{\tilde{F}}^T \mymat{\tilde{F}}
\end{equation}
where we have defined the centered matrix
\begin{equation}
  \mymat{\tilde{F}} \equiv \mymat{F} - \myvec{1}_N\myvec{\bar{f}}^T.
\end{equation}
and $\myvec{1}_N$ is the length-$N$ vector of ones.  The $N-1$ in the
denominator of Equation~\ref{eq:mat_corr_def} is called the
{\it Bessel Correction}, and results from the reduced number of degrees of
freedom after the mean is subtracted.

We can approximate the eigenfunctions $e^{(i)}(\lambda)$ and
eigenvalues $\lambda_i$ via the diagonalization 
of $\mymat{\mathcal{C}_F}$, computed using standard linear algebra techniques.
The diagonalization of the covariance matrix is
\begin{equation}
  \label{eq:evd_def}
  \mymat{\mathcal{C}_F} = \mymat{V}\mymat{\Lambda}\mymat{V}^T,
\end{equation}
where the columns of the matrix $\mymat{V}$ are the eigenvectors
(such that $\mymat{V}^T \mymat{V} = \mymat{I}$, the identity matrix), and
$\mymat{\Lambda}$ is the diagonal matrix of eigenvalues, such that
$\Lambda_{ij} = \lambda_i\delta_{ij}$.  In practice, the eigenvalues and
eigenvectors can often be computed more efficiently via a singular value
decomposition:
\begin{equation}
  \label{eq:svd_def}
  \mymat{U}\mymat{\Sigma}\mymat{V}^T
  = \frac{1}{\sqrt{N - 1}}\mymat{\tilde{F}}
\end{equation}
where the orthogonal matrices $\mymat{U}$ and $\mymat{V}$ are called the
left and
right singular vectors, respectively, and $\mymat{\Sigma}$ is a diagonal matrix
of singular values.  One can quickly show from Equations~\ref{eq:mat_corr_def}
and \ref{eq:svd_def} that
\begin{eqnarray}
  \mymat{\mathcal{C}_F}
  &=& \mymat{V}\mymat{\Sigma}\mymat{U}^T
  \mymat{U}\mymat{\Sigma}\mymat{V}^T\nonumber\\
  &=& \mymat{V}\mymat{\Sigma}^2\mymat{V}^T.
\end{eqnarray}
Comparing to Equation~\ref{eq:evd_def}, we see that 
$\mymat{\Lambda} = \mymat{\Sigma}^2$, and the eigenvectors $\mymat{V}$ are
identical to the right singular vectors of Equation~\ref{eq:svd_def},
up to an arbitrary ordering of columns.  Ordering our eigenvalues
according to the rule in \S\ref{sec:partial_recons} takes care of
this uncertainty.

The columns of $\mymat{V}$ are the {\it eigenvectors}, and are the discrete
representation of the eigenfunctions $e^{(i)}(\lambda)$.  These eigenfunctions
satisfy all the properties of the KL bases discussed above: they diagonalize
the sample correlation matrix, they provide the best possible low-rank
linear approximation to any spectrum from the sample, and in the presence of
uncorrelated noise, they allow an orthogonal decomposition with a natural
ranking in signal-to-noise.

In the case of the spectrum example, we have no theoretical expectation for the
correlation matrix $\mathcal{C}(\lambda, \lambda^\prime)$, so we are forced
to approximate the matrix based on the sample correlation using
Equation~\ref{eq:mat_corr_def}.  If we had sufficient physical understanding
of every process at work in each galaxy, it might be possible to compute
that correlation matrix from theory alone.  The number of variables involved,
however, make this prospect near impossible.

There are other situations in astronomical measurement, however, when a
theoretical expectation of the correlation matrix is possible in practice.
We will see in the following sections how the correlation matrix of
particular cosmological observations can be approximated from theory alone.


\subsection{KL with missing data}
Along with discrete data samples, another challenge when applying KL to
real data is the presence of missing data: given a KL basis, how can
one derive the projected coefficients when data are missing?
Note that in this section we'll assume that the KL basis has been
obtained independently.  For a discussion of how to approximate the
correlation matrix from data with missing values, refer to \citet{Yip04a}.

To begin, we'll assume that we have
an observed object represented by the $K$-dimensional vector 
$\myvec{x} = [x_1, x_2 \cdots x_K]^T$ and a set of normalized
KL basis functions 
$\mymat{V} = [\myvec{v_1}, \myvec{v_2} \cdots \myvec{v_K}]$.
arranged in order of decreasing eigenvalue $\lambda_i$.  We have shown above
that the best rank-$N$ linear approximation of $\myvec{x}$ is given by
\begin{equation}
  \myvec{x}^{(N)} = \sum_{i=1}^{N}a_i\myvec{v_i}.
\end{equation}
where the coefficients can be calculated as
\begin{equation}
  \label{eq:a_coeff_def}
  a_i = \myvec{v_i}^T\myvec{x}.
\end{equation}
When $\myvec{x}$ has missing data,
however, the question of how to compute $a_i$ is not as straightforward.
Simply setting the missing values to zero
will not work: the reconstruction will then faithfully recover those
zero values.  We desire instead to constrain the expected contribution
of each eigenvector to $x$ while {\it ignoring} the contribution of the
missing data.

A simple solution may be to simply truncate the vectors such that the dot
product is only computed over unmasked values.  It is easy to see that this
is identical to the  ``set to zero'' solution just
discussed.  Furthermore, there is the problem that in general, a set
of bases truncated in this way does not retain its orthogonality.

Another approach may be to derive a new basis which {\it is} orthogonal
over the truncated space.  This is similar in spirit to the method
explored by \citet{Gorski1994} in analysis of CMB data.
While this leads to a complete orthogonal basis for the observed portion
of the field, coefficients of these new modes have no simple relationship to
coefficients of modes covering the full field.  In general, a low-rank
transformation matrix must be inverted in order to convert between the two.

We'll use a different approach.  We have shown above that when noise is
not present, the KL vectors define the optimal basis for rank-$N$
reconstruction in the least-squares sense.  That is, for an arbitrary
truncated orthonormal basis $\mymat{\Phi}_{(N)}$, (where truncated means
we use only the first $N$ columns of $\mymat{\Phi}$),
\begin{equation}
  \chi^2 = \left|\left(\myvec{x}
  - \mymat{\Phi}_{(N)}^T\myvec{a}^{(N)}\right)\right|^2
\end{equation}
is minimized on average when
$\mymat{\Phi} = \mymat{V}_{(N)}
= [\myvec{v_1}, \myvec{v_2} \cdots \myvec{v_M}]$, $N \le M$.
Here we flip the problem: we know the desired basis $\mymat{V}_{(N)}$,
and hope to find the optimal vector of coefficients $\myvec{a}^{(N)}$
which minimizes the $\chi^2$ in the presence of the missing data
\citep{Connolly99}.
We'll define a 
diagonal weight matrix $\mymat{W}$ such that $W_{ij} = w_i\delta_{ij}$,
where $w_i=1$ where $x_i$ is defined, and $w_i=0$ where $x_i$ is
missing.  Our expression to minimize then becomes
\begin{equation}
  \chi^2(\myvec{a}^{(N)}) = \left(\myvec{x}
  - \sum_{i=1}^N a^{(N)}_i \myvec{v^{(i)}}\right)^T
  \mymat{W}^2 \left(\myvec{x}
  - \sum_{i=1}^N a_{(N),i} \myvec{v^{(i)}}\right).
\end{equation}
To minimize this with respect to the coefficients $a_{(N),i}$, we differentiate
and find
\begin{equation}
  \frac{\partial\chi^2}{\partial a_{(N),i}} = -2 \myvec{x}^t\mymat{W}^2\myvec{v^{(i)}}
  + 2\sum_{j=1}^N a_{(N),j} \myvec{v^{(j)}}^T \mymat{W}^2 \mymat{v^{(i)}}.
\end{equation}
Setting the derivative to zero and combining terms gives
\begin{equation}
  \label{eq:masked_coeff}
  \myvec{x}^T\mymat{W}^2\myvec{v^{(i)}} =
  \sum_{j=1}^N a^{(N)}_j \myvec{v^{(j)}}^T \mymat{W}^2 \mymat{v^{(i)}}.
\end{equation}
If there are no areas of missing data, then $\mymat{W} = \mymat{I}$ and we
simply recover $a_i = \myvec{v_i}^T\myvec{x}$, our standard expression
for finding the KL coefficients with no missing data.
Because the inner-product of
$\myvec{v^{(i)}}$ and $\myvec{v^{(j)}}$ is modulated by $\mymat{W}$, however,
there is no delta function to collapse the sum.

We can simplify this notation by defining the correlation matrix of the
mask $\mymat{M}_{(N)}$, such that
\begin{equation}
  [\mymat{M}_N]_{ij} \equiv \myvec{v^{(j)}}^T \mymat{W}^2 \mymat{v^{(i)}}
\end{equation}
so that eq.~\ref{eq:masked_coeff} can be compactly written
\begin{equation}
  \myvec{x}^T \mymat{W}^2 \mymat{V}_{(N)}
  = {\myvec{a}^{(N)}}^T \mymat{M}_N.
\end{equation}
From this, we can quickly see that the optimal set of coefficients
$\myvec{a}^{(N)}$ is given by
\begin{equation}
  \label{eq:a_truncated}
  \myvec{a}^{(N)} = [\mymat{M}_N]^{-1}
  \mymat{V}_{(N)}^T \mymat{W}^2 \myvec{x}.
\end{equation}
This is the equivalent of the expression in eq.~\ref{eq:a_coeff_def}:
if $\mymat{W}$ is set equal to the identity matrix (indicating no
missing data), then $\mymat{M}_N$ is also the identity and we recover
eq.~\ref{eq:a_coeff_def} exactly.

If some of the diagonal entries in $\mymat{W}$ are zero, then the
correlation matrix for the full set of $K$ eigenvectors is rank-deficient,
and cannot be inverted as required for eq.~\ref{eq:a_truncated}.  For this
reason, it is essential to use the truncated eigenvectors $\mymat{V}_{(N)}$,
with $N \le rank(\mymat{W})$, the rank of the matrix $\mymat{W}$.  For the
case here where $\mymat{W}$ is a diagonal matrix consisting of zeros and
ones, the rank is equivalent to the trace, or the sum of nonzero diagonal
terms.

Once these approximate KL coefficients $\myvec{a}^{(N)}$ are determined,
it is straightforward to use these to approximate the unmasked vector
$\myvec{x}$:
\begin{equation}
  \myvec{x} \approx \sum_{i=0}^N a_i^{(N)} \myvec{v^{(i)}}.
\end{equation}
Here we have used the coefficients determined from the unmasked region of
the data to constrain the unobserved value in the masked regions.

This could be further generalized by allowing $\mymat{W}$ to be an arbitrary
matrix, for instance encoding the inverse of the noise covariance
associated with the observed vector $\myvec{x}$.  In unconstrained regions,
the noise is infinite and the inverse is zero.  This leads to very similar
results to those expressed here (in this case, $\mymat{W}^2$ is replaced
by $\mymat{W}^T\mymat{W}$).  This is equivalent to the whitening operation
discussed in \S\ref{sec:whitening}.  We will not use this formalism here, so we
leave it only as a suggested extension.

\section{\KL\ Analysis and Bayesian Inference}
Because of the signal-to-noise optimality properties of KL, it can be
very useful within Bayesian parameter estimation.  
Given observations $D$ and prior information $I$, Bayes' theorem specifies the
posterior probability of a model described by the parameters $\{\theta_i\}$:
\begin{equation}
  \label{eq:bayes}
  P(\{\theta_i\}|DI) = P(\{\theta_i\}|I) \frac{P(D|\{\theta_i\}I)}{P(D|I)}
\end{equation}
The term on the left hand side
is the \textit{posterior} probability of the set of
model parameters $\{\theta_i\}$, which is the quantity we are interested in.

The first term on the RHS is the \textit{prior}.  It quantifies how our prior
information $I$ affects the probabilities of the model parameters.  The 
prior is where information from other surveys (e.g. WMAP, etc) can be
included. The likelihood function for the observed coefficients $\myvec{a}$
enters into the numerator $P(D|\{\theta_i\}I)$.  The denominator $P(D|I)$
is essentially a normalization constant, set so that the sum of probabilities
over the parameter space equals unity.

KL is useful in the case where the model $\{\theta\}$ can be expressed in
terms of a covariance $\mymat{\mathcal{C}}_{\{\theta\}}$ \citep[see][]{Vogeley96}.
Given a data vector $\mymat{d}$ with observed noise covariance
$\mymat{\mathcal{N}}$, the KL vectors are the eigenvectors
$\mymat{V}_{\{\theta\}}$ of the whitened total covariance 
\begin{equation}
  \mymat{\mathcal{C}}_{\{\theta\}, W} =
  \mymat{\mathcal{N}}^{-1/2}
  \mymat{\mathcal{C}}_{\{\theta\}}
  \mymat{\mathcal{N}}^{-1/2}.
\end{equation}
These eigenvectors can be used to quickly compute the KL coefficients of
the observed data,
\begin{equation}
  \myvec{a}_{\{\theta\}} = \mymat{V}^\dagger_{\{\theta\}}
  \mymat{\mathcal{N}}^{-1/2}\myvec{d}
\end{equation}
For a given model $\{\theta_i\}$, we can predict the expected
distribution of coefficients $\myvec{a}_{\{\theta_i\}}$:
\begin{eqnarray}
  \mymat{X}_{\{\theta_i\}}
  & \equiv & \langle\myvec{a}_{\{\theta_i\}}
  \myvec{a}_{\{\theta_i\}}^\dagger\rangle\nonumber\\
  &=& \mymat{V}_{\{\theta\}}^\dagger \mymat{\mathcal{N}}^{-1/2} 
  \mymat{\mathcal{C}}_{\{\theta_i\}}\mymat{\mathcal{N}}^{-1/2}\mymat{V}_{\{\theta\}}
  + \mymat{I}.
\end{eqnarray}
If the length of the data vector $\myvec{d}$ is $N$, then the full
analysis results in $\mymat{X}_{\{\theta_i\}}$ being an $N\times N$ matrix.
Alternatively, one can truncate the eigenvectors to $n < N$ terms:
this leads to $\mymat{X}_{\{\theta_i\}}$ being an $n\times n$ matrix, and
is equivalent to working with an optimal low-rank approximation of the
data $\myvec{d}$.
Using this, the measure of departure from the model $\{\theta\}$
is given by the quadratic form
\begin{equation}
  \chi_{\{\theta\}}^2 = \myvec{a}^\dagger\mymat{X}_{\{\theta_i\}}^{-1}\myvec{a}
\end{equation}
The likelihood is then given by
\begin{equation}
  \label{eq:likelihood}
  P(D|\{\theta\}I) =
  \mathcal{L}(\myvec{a}|\{\theta_i\}) = 
  (2\pi)^{n/2} |\det(X_{\{\theta_i\}})|^{-1/2}
  \exp(-\chi_{\{\theta\}}^2/2)
\end{equation}
where $n$ is the number of degrees of freedom: that is, the number
of eigenmodes included in the analysis.  The likelihood given by
Equation~\ref{eq:likelihood} enters into Equation~\ref{eq:bayes} when
computing the posterior probability.  This sort of approach will be
applied to observed shear in Chapter 5.

\section{\KL\ Analysis of Shear}
In the case of shear observations, the observed vector $\myvec{\gamma}$
consists of the $N$ ellipticity observations within the series of window
functions $A_i(\myvec{\theta}, z)$, with $i = 1...N$.  In general, these
window functions can overlap.  The expected correlation matrix of the
observed shear $\myvec{\gamma}$ is given by
\begin{equation}
  [\mymat{\mathcal{C}_\gamma}]_{ij}
  = \int \dd^2\theta A_i(\myvec{\theta}, z)
  \int \dd^2\theta^\prime A_j(\myvec{\theta}^\prime, z)
  \xi_+(|\myvec{\theta} - \myvec{\theta}^\prime|) + \mymat{\mathcal{N}}_{ij}.
\end{equation}
where the matrix $\mymat{\mathcal{N}}_{ij}$ is the noise covariance between
bins.  The shear correlation function $\xi_+$ can be computed from the
theoretical 3D mass power spectrum, using the results from the previous
chapter (eqs.~\ref{eq:xi_plus}-\ref{eq:lensing_weight}).

The noise matrix $\mymat{\mathcal{N}}$ can be estimated from the measurement
process: in the simplest case where noise is due to shot-noise only and
the windows $A_i$ are non-overlapping,
$\mymat{\mathcal{N}} \propto \sigma_i^2 \mymat{I}$
where $\sigma_i = \sigma_{int} / \sqrt{N_i}$ is the shot noise, given
the intrinsic ellipticity $\sigma_{int}$ and the number of galaxies $N_i$
in the bin described by $A_i$.

