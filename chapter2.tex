\chapter{Introduction to Karhunen-Lo\`{e}ve Analysis}

%\begin{itemize}
%  \item Introduce the notational formalism
%  \item KL as an eigenvalue problem
%  \item KL as a signal-to-noise ranking
%  \item KL as an optimal low-dimensional representation
%  \item KL on noisy data
%  \item KL with missing information
%\end{itemize}

%Also mention LLE and cite \cite{Vanderplas2009}, which uses a nonlinear
%alternative to PCA for dimensionalty reduction.

\KL\ (KL) analysis is a commonly used statistical tool
in a broad range of astronomical applications, from, e.g.~studies of 
galaxy and quasar spectra \citep{Connolly95,Connolly99,Yip04a,Yip04b}, to 
analysis of the spatial distribution of galaxies 
\citep{Vogeley96,Matsubara00,Pope04}, to characterization of the 
expected errors in weak lensing surveys \citep{Kilbinger06, Munshi06}.
In this chapter, we will develop the formalism of KL which will form
the basis of the applications in the subsequent chapters.

The KL formalism requires the liberal
employment of algebra with vectors, scalars, matrices, and their
generalizations.
For clarity, we will begin by briefly specifying the notational
conventions used in this chapter and throughout this work.

\section{Notational Conventions}

It is important to clearly distinguish between vectors, matrices, and
scalars in the following formulation.  Vectors will be denoted by
bold lower-case symbols; e.g. $\myvec{x}$.  Matrices will be denoted by
bold upper-case symbols; e.g. $\mymat{X}$.  Scalars will be denoted by
non-bold symbols, either upper or lower-case.
All vectors are assumed to be column vectors, while a row-vector is
indicated by the transpose, $\myvec{x}^T$.  
Single elements of a given
vector or matrix are given with subscripts: $x_i$ is the $i^{\rm th}$
element of the vector $\myvec{x}$, and $X_{ij}$ is the element in the
$i^{\rm th}$ row and $j^{\rm th}$ column of the matrix $\mymat{X}$.
The column vector making up the $j^{\rm th}$ row of $\mymat{X}$ is
indicated by $\myvec{x}^{(j)}$.  Note then, that by this convention,
the $(i, j)$ component of matrix $\mymat{X}$ can be equivalently expressed
$X_{ij}$ or $x_i^{(j)}$.

In algebraic expressions, the normal linear algebra rules are assumed.
For example, the expression 
\begin{equation}
  \myvec{y} = \mymat{M}\mymat{x} + \mymat{b}
\end{equation}
involves the vectors $\myvec{y}$, $\myvec{x}$, and $\myvec{b}$ and the
matrix $\mymat{M}$.  This expression is short-hand for the summation:
\begin{equation}
  y_i = \sum_{j=1}^{n} M_{ij} x_j + b_i
\end{equation}
Using these rules, we can define the magnitude of a vector
\begin{equation}
  |\myvec{x}| \equiv \sqrt{\myvec{x}^T \myvec{x}}
\end{equation}

\section{Basis function decomposition}
KL analysis is simply a basis function decomposition, where the basis
functions are derived based on the variance and covariance properties
of a class of functions.  We'll start by describing what is perhaps the
best-known basis function decomposition, the Fourier Series.
We start here because it's a familiar concept which generalizes well to
the fundamental ideas of KL analysis.

\subsection{Fourier Series}
The Fourier Series is a means of expressing a bounded function in terms
of a certain class of oscillatory basis functions.  It is a discrete version
of the Fourier Transforms used in cosmological power spectrum analysis, and
discussed in section XXX.

We'll define a set of oscillatory basis functions 
\begin{equation}
  \label{eq:fourier_basis}
  \Phi_k(t) = \frac{1}{\sqrt{b - a}}
  \exp\left[\frac{2\pi i k (t-a)}{b - a}\right].
\end{equation}
and postulate that a function $f(t)$ can be expressed as a linear combination
of these basis functions:
\begin{equation}
  \label{eq:fourier_1D}
  f(t) = \sum_{k=-\infty}^\infty a_k\Phi_k(t).
\end{equation}
Here $a_k$ are an infinite set of complex coefficients.  Our claim is that
any piecewise continuous and square-integrable function $f(t)$ in the
interval $[a, b]$ can be represented this way.  A rigorous mathematical proof
of this statement can be found elsewhere, but below we will lend support
to this claim.

Given the claim that Equation~\ref{eq:fourier_1D} holds, how can we
compute the Fourier coefficients $a_k$ associated with 
a particular $f(t)$?  Though the expression is well-known, we'll briefly
derive it here because it illuminates some of the properties of
Fourier transforms which will generalize to KL transforms.

To begin, we'll multiply both sides of Equation~\ref{eq:fourier_1D} by the
complex conjugate $\Phi^\ast_{k^\prime}(x)$ of the basis function given
in Equation~\ref{eq:fourier_basis}, and integrate both sides over $t$
from $a$ to $b$:
\begin{equation}
  \int_a^b \Phi^\ast_{k^\prime}(t) f(t) \dd t
  = \int_a^b \Phi^\ast_{k^\prime}(t) \sum_{k=-\infty}^\infty a_k\Phi_k(x)\dd t
\end{equation}
On the right-hand side, we can exchange the order of integration and
summation to find 
\begin{equation}
  \label{eq:fourier_coef_der}
  \int_a^b \Phi^\ast_{k^\prime}(t) f(t) \dd t
  =  \sum_{k=-\infty}^\infty a_k \left[\int_a^b \Phi^\ast_{k^\prime}(t)\Phi_k(x)\dd t\right].
\end{equation}
Let's examine the term in the square brackets.  Plugging in the definition
of the basis functions from Equation~\ref{eq:fourier_basis}, we have
\begin{equation}
  \left[\int_a^b \Phi^\ast_{k^\prime}(t)\Phi_k(x)\dd t\right]
  = \left[\frac{1}{b-a}\int_a^b \exp\left(\frac{2\pi i (k - k^\prime) (t-a)}
    {b - a}\right)\dd t\right]
\end{equation}
This gives two distinct situations for the integral on the right-hand side:
when $k=k^\prime$, both the integrand 
and the term in the brackets is exactly $1$.  When $k\ne k^\prime$,
the integrand oscillates through an integer number of cycles between
$a$ and $b$ (remember that $k$ and $k^\prime$ here are integers),
and the result of the integral is exactly $0$.
So we see that the term in brackets is equal to simply 
the Kronecker delta function $\delta_{kk^\prime}$, defined as
\begin{equation}
  \delta_{ij} = \left\{
  \begin{array}{ll}
    1 & {\rm if}\ i = j\\
    0 & {\rm if}\ i \ne j.
  \end{array}
  \right.
\end{equation}
Putting this result into Equation~\ref{eq:fourier_coef_der}, only one
term of the sum remains and we find
\begin{equation}
  \label{eq:fourier_coef}
  a_k = \frac{1}{b - a}\int_a^b \Phi^\ast_k(t) f(t) \dd t
\end{equation}

Equation~\ref{eq:fourier_coef} shows how to compute the Fourier coefficients
$a_k$ for a given $f(t)$.  But one might wonder if this is a unique result.
Could there be several possible sets of valid Fourier coefficients for
a given function?

Let's assume that given a function $f(t)$, there are two valid sets of
Fourier coefficients $a_k$ and $a^\prime_k$ which satisfy
Equation~\ref{eq:fourier_1D}.  In this case, subtracting the two equations
gives
\begin{equation}
  f(t) - f(t) = \sum_{k=-\infty}^\infty (a_k - a^\prime_k)\Phi_k(t) = 0.
\end{equation}
In a similar manner to above, we can multiply by $\Phi^\ast_k(t)$, integrate
over $t$ from $a$ to $b$, and extract a kronecker delta function to
yield
\begin{equation}
  \sum_{k=-\infty}^\infty (a_k - a^\prime_k) \delta_{kk^\prime} = 0
\end{equation}
Collapsing the sum, we find that $a_k = a^\prime_k$ for all $k$.  This
shows the uniqueness of the fourier coefficients $a_k$ for a given function
$f(t)$ on an interval $[a, b]$.

\subsection{Generalizing Orthonormal Bases}

Stepping back for a moment, we have shown that for a particular
class of functions $\Phi_k(t)$, one can find unique coefficients
$a_k$ such that one of the expansions of Equation~\ref{eq:fourier_1D} holds.
A key observation is that all the derivations above rested 
soley on two special properties of these basis functions:
\begin{enumerate}
  \item
    The basis functions are {\it orthonormal} on the interval $[a, b]$.
    That is, $\Phi_k(t)$ satisfies
    \begin{equation}
      \int_a^b \Phi_k(t) \Phi^\star_{k^\prime}(t) = \delta_{kk^\prime}
    \end{equation}
  \item
    The basis functions are {\it complete} on the interval $[a, b]$.
    That is, for every $t_0$ satisfying $a \le t_0 \le b$, there exists
    a $k$ such that
    \begin{equation}
      \Phi_k(t_0) \ne 0.
    \end{equation}
    for at least one choice of $k$.
\end{enumerate}
As long as these two properties hold for a class of functions $\Phi_k(t)$,
we would be able to repeat the above derivations and express any $f(t)$
via Equation~\ref{eq:fourier_1D}.
This suggests the possible existence of other functions which fit these
criteria.  Some examples are the Legendre polynomials on the interval
$[-1, 1]$, the Laguerre polynomials on the interval $[0, \infty)$, and
the Hermite polynomials on the interval $(-\infty, \infty)$.
In fact, just as there are an infinite number of possibile orientations for
an $(x, y)$ axis in a two-dimensional vector space, there are an infinite
number of possible orthogonal function bases that work in the above
formalism.  Choosing the right basis can lead to a much easier analysis of
a given problem.

\section{\KL\ Analysis}
Because of the infinite number of possible orthogonal function classes,
one might wonder how to choose the optimal class for any particular problem.
\KL\ Analysis Seeks to answer this question in a very general case.

\subsection{Derivation of \KL\ theorem}
Imagine now that we have a random process $F_t$.  This can be thought of as
an arbitrarily large collection of functions $f^{(i)}(t)$ defined
on the interval
$t \in [a, b]$.  At a given location $t$, the expectation value of the
random process is given by
\begin{equation}
  E[F_t] = \lim_{N \to \infty} \frac{1}{N} \sum_{i=1}^N f^{(i)}(t)
\end{equation}
For simplicity, we'll assume that the random process $F_t$ is {\it
centered}; that is $E[F_t] = 0$.  A general random process can be
centered by subtracting the expectation value for each $t$.
A random process like $F_t$ can be characterized
by its covariance function, which is defined as
\begin{eqnarray}
  \label{eq:corrfunc_def}
  \mathcal{C}_F(t, t^\prime) &\equiv& E[F_t F_{t^\prime}].
  \nonumber\\
  &=& \lim_{N\to\infty}\frac{1}{N} \sum_{i=1}^N
  f^{(i)}(t)f^{(i)}(t^\prime)
\end{eqnarray}
where we have 
For an {\it uncorrelated} random process,
$\mathcal{C}_F(t, t^\prime) = \mathcal{C}_F(t, t) \delta_{t, t^\prime}$
where $\mathcal{C}_F(t, t) $ is called the variance of $F_t$.

\subsection{Eigenfunctions}
We'll now introduce the {\it eigenfunctions} $e_k(t)$
of the covariance function $\mathcal{C}_F(t, t^\prime)$, which satisfy
\begin{equation}
  \label{eq:eigfunc_def}
  \int_a^b \mathcal{C}_F(t, t^\prime) e_k(t^\prime)\dd t^\prime
  = \lambda_k e_k(t)
\end{equation}
subject to the constraint that $e_k(t)$ is not everywhere zero.
Here $\lambda_k$ is the {\it eigenvalue} associated with the
eigenfunction $e_k(t)$.

Now what are the properties of these eigenfunctions?  First of all, they
are orthogonal on the interval $[a, b]$.
We can show this by considering two arbitrary eigenfunctions
$e_k(t)$ and $e_{k^\prime}(t)$.  Consider the quantity
\begin{equation}
  \int_a^b \dd t \int_a^b \dd t^\prime \mathcal{C}_F(t, t^\prime)
  e_{k^\prime}(t^\prime) e_k(t)
\end{equation}
Because $\mathcal{C}_F(t, t^\prime) = \mathcal{C}_F(t, t^\prime)$, and
because the order of integration can be switched, this can be evaluated
two different ways, which must be equal:
\begin{eqnarray}
  \int_a^b \dd t e_k(t)
  \int_a^b \dd t^\prime \mathcal{C}_F(t, t^\prime) e_{k^\prime}(t^\prime) &=&
  \int_a^b \dd t^\prime e_k^\prime(t^\prime)
  \int_a^b \dd t \mathcal{C}_F(t^\prime, t) e_{k}(t)
  \nonumber\\
  \int_a^b \dd t \lambda_{k^\prime} e_k(t) e_{k^\prime}(t) &=&
  \int_a^b \dd t^\prime \lambda_k e_k(t^\prime) e_{k^\prime}(t^\prime)
\end{eqnarray}
Rearranging the bottom line leads to
\begin{equation}
  (\lambda_k - \lambda_{k^\prime})
  \int_a^b \dd t e_k(t) e_{k^\prime}(t) = 0
\end{equation}
So for $\lambda_k \ne \lambda_{k^\prime}$, then $e_k$ and $e_{k^\prime}$
must be orthogonal\footnote{In the degenerate case when
$\lambda_k = \lambda_{k^\prime}$, one can still construct orthogonal
vectors by linear combinations:
\begin{eqnarray}
  e_+(t) &=& \frac{e_k(t) + e_{k^\prime}(t)}{\sqrt{2}} \nonumber\\
  e_-(t) &=& \frac{e_k(t) - e_{k^\prime}(t)}{\sqrt{2}}. \nonumber
\end{eqnarray}
This leads to two new orthogonal eigenfunctions $e_+$ and $e_-$ with the
same eigenvalue $\lambda_k$.}.  From the definition in
Equation~\ref{eq:eigfunc_def}, we see that if $e_k(t)$ is an eigenfunction
with eigenvalue $\lambda_k$,
then $C\, e_k(t)$ is an eigenfunction with eigenvalue $\lambda_k$ as well.
To make the choice of eigenfunction more definite, we will assume all
eigenfunctions are normalized: that is
\begin{equation}
  \int_a^b \dd t e_k(t) e_{k^\prime}(t) = \delta_{kk^\prime}
\end{equation}
for all $k$.  This still allows any eigenfunction to have an arbitrary
phase: that is, an eigenfunction may be multiplied by $e^{i\theta}$ for
any theta, and still satisfy our orthogonality condition.  This fact
will become important later.

The net result is that the eigenfunctions $e_k(t)$ form an orthonormal
basis for the space of functions represented by the random process $F_t$.
In general, the eigenfunctions
also satisfy the completeness relation: that is, 
an arbitrary function $f(t)$ can be approximated by
\begin{equation}
  f(t) = \sum_{k=1}^N a_k e_k(t)
\end{equation}
and the mean square error satisfies
\begin{equation}
  \lim_{N\to\infty} \int_a^b
  \left[f(t) - \sum_{k=1}^{N}a_k e_k(t)\right]^2 \dd t = 0.
\end{equation}
The proof of the completeness of eigenfunctions for a symmetric kernel
$\mathcal{C}_F(t^\prime, t)$ can be found in, e.g.~\citet{Courant1989}.

Let's now consider the expansion of the random process $F_t$ onto the
eigenvectors $e_k(t)$.  Analogously to the Fourier case discussed
above, we have
\begin{equation}
  \label{eq:Ft_decomp}
  F_t = \sum_{k=1}^\infty A_k e_k(t).
\end{equation}
where here $A_k$ can be thought of as a set of coefficients $a_k^{(i)}$
in the same way that the random process $F_t$ can be thought of as a
set of functions $f^{(i)}(t)$.
Multiplying both sides by $e_{k^\prime}(t)$, integrating, and using the
orthogonality of eigenvectors (this is analogous to the derivation
in Equations~\ref{eq:fourier_coef_der}-\ref{eq:fourier_coef}) leads to
\begin{equation}
  \label{eq:F_k_def}
  A_k = \int_a^b F_t e_k(t) \dd t
\end{equation}

Because of the fact that the random process is centered,
 $E[F_t] = 0$ and
is straightforward to show that $E[A_k] = 0$ as well.  The more interesting
computation is that of the covariance $\mathcal{C}_A(k, k^\prime)$.
From Equations~\ref{eq:corrfunc_def} and \ref{eq:F_k_def}, we have
\begin{eqnarray}
  \mathcal{C}_A(k, k^\prime)
  &=& E[A_k A_{k^\prime}]\nonumber\\
  &=& E\left[\int_a^b \dd t \int_a^b \dd t^\prime
    {F}_t e_k(t)
    {F}_{t^\prime} e_{k^\prime}(t^\prime)\right] \nonumber\\
  &=& \int_a^b \dd t \int_a^b \dd t^\prime
    E[{F}_t {F}_{t^\prime}]
    e_{k^\prime}(t^\prime) e_k(t) \nonumber\\
  &=& \int_a^b \dd t \int_a^b \dd t^\prime
    \mathcal{C}_F(t, t^\prime)
    e_{k^\prime}(t^\prime) e_k(t) \nonumber
\end{eqnarray}
Substituting Equation~\ref{eq:eigfunc_def}, we find that this gives
\begin{eqnarray}
  \label{eq:cov_A}
  \mathcal{C}_A(k, k^\prime)
  &=& \int_a^b \dd t \lambda_{k^\prime} e_{k^\prime}(t) e_k(t) \nonumber\\
  &=& \lambda_k \delta_{kk^\prime}
\end{eqnarray}
So we see that projection of the centered random process $\tilde{F}_t$ onto
the eigenvectors of its covariance matrix yields coefficients which
are uncorrelated, with variance equal to the eigenvalues $\lambda_k$.
This result is the \KL\ theorem, and it has many ramifications that will
be discussed below.

\subsection{Partial Reconstructions}
\label{sec:partial_recons}
We have shown that \KL\ provides an orthonormal basis for a random field
with uncorrelated projection coefficients.  We can go further and show
that \KL\ provides the
optimal orthonormal basis for low-rank approximations of functions in
a random field.

Let us consider an arbitrary complete orthonormal basis
$\phi_k(t)$, with $F_t = \sum_{k=1}^\infty B_k\phi_k(t)$.
Given this basis, we'll define the low-rank approximation of $F_t$
\begin{equation}
  F^{(N)}_t = \sum_{k=1}^N B_k\phi_k(t)
\end{equation}
We'll seek to minimize the expectation value of the squared error
\begin{eqnarray}
  \mathcal{E}^2_N &=& \int_a^b \left[F_t - F_t^{(N)}\right]^2 \dd t
  \nonumber\\
  &=& \int_a^b \left[\sum_{k = N + 1}^\infty B_k \phi_k(t)\right]^2 \dd t.
\end{eqnarray}


Expanding the sum leads to
\begin{eqnarray}
  \mathcal{E}^2_N &=& \sum_{k=N + 1}^\infty
  \sum_{k^\prime=N + 1}^\infty \int_a^b 
  \left[B_k B_{k^\prime} \phi_k(t) \phi_{k^\prime}(t)\right] \dd t
  \nonumber\\
  &=& \sum_{k=N+1}^\infty \sum_{k^\prime=N+1}^\infty B_k B_{k^\prime}
  \delta_{kk^\prime}\nonumber\\
  &=& \sum_{k=N+1}^\infty B_k^2
\end{eqnarray}
Taking the expectation value and plugging in the equivalent of
Equation~\ref{eq:F_k_def} for $B_k$, we find
\begin{eqnarray}
  E[\mathcal{E}^2_N] &=&
  E\left[\sum_{k=N+1}^\infty \int_a^b \dd t \int_a^b \dd t^\prime
  F_tF_{t^\prime} \phi_m(t)\phi_m(t^\prime)\right] \nonumber\\
  &=& \sum_{k=N+1}^\infty \int_a^b \dd t \int_a^b \dd t^\prime
  C_F(t, t^\prime) \phi_m(t)\phi_m(t^\prime)
\end{eqnarray}

We'd like to minimize this expected error over the basis $\phi_m(t)$, subject
to the constraint that $\phi_m(t)$ are an orthonormal basis.  We'll accomplish
this by the method of Lagrange multipliers.  Our Lagrangian is
\begin{equation}
  \mathcal{L}(\{\phi(t)\}) = \sum_{k=N+1}^\infty
  \left[\int_a^b \dd t \int_a^b \dd t^\prime
  C_F(t, t^\prime) \phi_m(t)\phi_m(t^\prime)
  - \lambda_k\left(1 - \int_a^b \phi_m(t)^2 \dd t\right)
  \right]
\end{equation}
Minimizing this with respect to $\phi_k$ gives
\begin{equation}
  \frac{\partial\mathcal{L}}{\partial\phi_k(t)} = \int_a^b \dd t^\prime
  C_F(t, t^\prime) \phi_k(t^\prime)
  - \lambda_k \phi_k(t).
\end{equation}
The optimumum for each $k$ is where this derivative equals zero; setting to
zero and solving recovers the original eigenvalue problem
 (Eq. \ref{eq:eigfunc_def}) from which we
derived the KL basis $\{e_k(t)\}$.  By the uniqueness of the eigenvalue
decomposition, this shows that the KL basis is the optimal basis for low-rank
approximations of functions drawn from $F_t$.  Furthermore, for an
approximation using $N$ eigenvectors, the mean squared error is given by
\begin{eqnarray}
  E[\mathcal{E}^2_N] &=& \sum_{k=N+1}^\infty E[A_k^2]\nonumber\\
  &=& \sum_{k=N+1}^\infty \lambda_k
\end{eqnarray}
This is an interesting result: it says that in order to minimize the
expectation value of the reconstruction error $\mathcal{E}_N$ for all $N$,
we simply need to order the eigenvalues such that $\lambda_k \ge \lambda_{k+1}$
for all eigenvalue-eigenfunction pairs $(\lambda_k, e_k(t))$.

Because of this, throughout this work we will follow this convention when
ordering the eigenvalues in and KL decomposition.

\subsection{KL in the presence of noise}
\label{sec:whitening}
In practice, the observed random field $F_t$ is composed of the sum of
a signal $S_t$ and noise $N_t$.  We'll continue to assume that both of these
are centered.  The covariance matrix then becomes
\begin{equation}
  \mathcal{C}_F(t, t^\prime) = E[(S_t + N_t)(S_{t^\prime}+N_{t^\prime})]
\end{equation}
Under the assumption that the signal $S_t$ and noise $N_t$ are uncorrelated,
this can be simplified to
\begin{eqnarray}
  \mathcal{C}_F(t, t^\prime) &=& E[S_t S_{t^\prime}]
  + E[N_t N_{t^\prime}] \nonumber\\
  &=& \mathcal{C}_S(t, t^\prime) + \mathcal{C}_N(t, t^\prime)
\end{eqnarray}
The \KL\ eigenfunctions always diagonalize the full covariance
$\mathcal{C}_F(t, t^\prime)$.
In the case of uncorrelated ``white'' noise,
$\mathcal{C}_N(t, t^\prime) = \sigma^2 \delta(t - t^\prime)$
and both the signal and the noise become diagonalized.  In this case,
the noise per mode is a constant $\sigma^2$, and the ranking of the
eigenfunctions leads to modes which are ranked in signal-to-noise.
This is why KL modes are often referred to as ``signal-to-noise
eigenmodes'' \citep{Vogeley96}.

In cases when the noise is not white, we can still recover signal-to-noise
information by preprocessing with a whitening transformation.  The eigenvectors
of the noise covariance, which satisfy
\begin{equation}
  \int_a^b \mathcal{C}_N(t, t^\prime) n_k(t^\prime) \dd t^\prime 
  = \sigma_i n_k(t)
\end{equation}
can be used to apply a whitening transform to both the signal and the
noise.  The whitened covariance is given by
\begin{eqnarray}
  \mathcal{C}_F^{(W)}(k, k^\prime) &\equiv& \int_a^b\dd t\int_a^b\dd t^\prime
  \mathcal{C}_F(t, t^\prime)
  \frac{n_k(t) n_{k^\prime}(t^\prime)}{\sigma_k\sigma_{k^\prime}}\nonumber\\
  &=& \mathcal{C}_S^{(W)}(k, k^\prime) + \delta_{kk^\prime}.
\end{eqnarray}
The signal is now expressed in the basis of the noise eigenmodes $n_k(t)$,
and the noise has been made white with unit variance.  The KL modes derived
from the whitened covariance $\mathcal{C}_F^{(W)}(k, k^\prime)$ will have
eigenvalues which represent the signal-to-noise in each mode.

\subsection{\KL: theory to practice}
The abstract formalism presented above is interesting in itself, but one
might wonder what practical advantages can be gained from this discussion.
In practice, we don't deal with an abstract stochastic process $F_t$, but
with discrete, measured data.  In this section we will discuss the
practical computational aspects of KL analysis.

Imagine, for the moment, that an astronomer has observed the spectra of
$N$ galaxies.  After normalization and correction for redshift effects,
the spectra can be encoded as a series of $N$ real-valued functions
$f^{(i)}(\lambda)$ over some defined domain $\lambda \in [a, b]$.
In practice, we measure these spectra at a finite set of wavelengths
$\myvec{\lambda}^T = [\lambda_1, \lambda_2 \cdots \lambda_M]$ so that
our observations $\myvec{f}^{(i)}$ become $M$-dimensional vectors.  For
convenience, we'll store these spectra in a $M \times N$ matrix
$\mymat{F} = [\myvec{f}^{(1)}, \myvec{f}^{(2)} \cdots \myvec{f}^{(N)}]^T$,
where each row of the matrix represents one spectrum.
These series of spectra $\mymat{F}$ 
can be considered a finite realization of a particular
random process $F_\lambda$.

The expectation value $E[F_\lambda]$ can be approximated via the sample mean:
\begin{equation}
  E[F_\lambda] \approx \myvec{\bar{f}}
  = \frac{1}{N} \sum_{i=1}^N \myvec{f}^{(i)}
\end{equation}
and the covariance function can be approximated by the covariance matrix
\begin{equation}
  \label{eq:mat_corr_def}
  E[F_\lambda F_{\lambda^\prime}] \approx 
  \mymat{\mathcal{C}_F} = \frac{1}{N - 1} \mymat{\tilde{F}}^T \mymat{\tilde{F}}
\end{equation}
where we have defined the centered matrix
\begin{equation}
  \mymat{\tilde{F}} \equiv \mymat{F} - \myvec{1}_N\myvec{\bar{f}}^T.
\end{equation}
and $\myvec{1}_N$ is the length-$N$ vector of ones.  The $N-1$ in the
denominator of Equation~\ref{eq:mat_corr_def} is called the
{\it Bessel Correction}, and results from the reduced number of degrees of
freedom after the mean is subtracted.

We can approximate the eigenfunctions $e^{(i)}(\lambda)$ and
eigenvalues $\lambda_i$ via the diagonalization 
of $\mymat{\mathcal{C}_F}$, computed using standard linear algebra techniques.
The diagonalization of the covariance matrix is
\begin{equation}
  \label{eq:evd_def}
  \mymat{\mathcal{C}_F} = \mymat{V}\mymat{\Lambda}\mymat{V}^T,
\end{equation}
where the columns of the matrix $\mymat{V}$ are the eigenvectors
(such that $\mymat{V}^T \mymat{V} = \mymat{I}$, the identity matrix), and
$\mymat{\Lambda}$ is the diagonal matrix of eigenvalues, such that
$\Lambda_{ij} = \lambda_i\delta_{ij}$.  In practice, the eigenvalues and
eigenvectors can often be computed more efficiently via a singular value
decomposition:
\begin{equation}
  \label{eq:svd_def}
  \mymat{U}\mymat{\Sigma}\mymat{V}^T
  = \frac{1}{\sqrt{N - 1}}\mymat{\tilde{F}}
\end{equation}
where the orthogonal matrices $\mymat{U}$ and $\mymat{V}$ are called the
left and
right singular vectors, respectively, and $\mymat{\Sigma}$ is a diagonal matrix
of singular values.  One can quickly show from Equations~\ref{eq:mat_corr_def}
and \ref{eq:svd_def} that
\begin{eqnarray}
  \mymat{\mathcal{C}_F}
  &=& \mymat{V}\mymat{\Sigma}\mymat{U}^T
  \mymat{U}\mymat{\Sigma}\mymat{V}^T\nonumber\\
  &=& \mymat{V}\mymat{\Sigma}^2\mymat{V}^T.
\end{eqnarray}
Comparing to Equation~\ref{eq:evd_def}, we see that 
$\mymat{\Lambda} = \mymat{\Sigma}^2$, and the eigenvectors $\mymat{V}$ are
identical to the right singular vectors of Equation~\ref{eq:svd_def},
up to an arbitrary ordering of columns.  Ordering our eigenvalues
according to the rule in \S\ref{sec:partial_recons} takes care of
this uncertainty.

The columns of $\mymat{V}$ are the {\it eigenvectors}, and are the discrete
representation of the eigenfunctions $e^{(i)}(\lambda)$.  These eigenfunctions
satisfy all the properties of the KL bases discussed above: they diagonalize
the sample correlation matrix, they provide the best possible low-rank
linear approximation to any spectrum from the sample, and in the presence of
uncorrelated noise, they allow an orthogonal decomposition with a natural
ranking in signal-to-noise.

In the case of the spectrum example, we have no theoretical expectation for the
correlation matrix $\mathcal{C}(\lambda, \lambda^\prime)$, so we are forced
to approximate the matrix based on the sample correlation using
Equation~\ref{eq:mat_corr_def}.  If we had sufficient physical understanding
of every process at work in each galaxy, it might be possible to compute
that correlation matrix from theory alone.  The number of variables involved,
however, make this prospect near impossible.

There are other situations in astronomical measurement, however, when a
theoretical expectation of the correlation matrix is possible in practice.
We will see in the following sections how the correlation matrix of
particular cosmological observations can be approximated from theory alone.


\subsection{KL with missing data}
Along with discrete data samples, another challenge when applying KL to
real data is the presence of missing data: given a KL basis, how can
one derive the projected coefficients when data is missing?
Note that in this section we'll assume that the KL basis has been
obtained independently.  For a discussion of how to approximate the
correlation matrix from data with missing values, refer to \citet{Yip04a}.

To begin, we'll assume that we have
an observed object represented by the $K$-dimensional vector 
$\myvec{x} = [x_1, x_2 \cdots x_K]^T$ and a set of normalized
KL basis functions 
$\mymat{V} = [\myvec{v_1}, \myvec{v_2} \cdots \myvec{v_K}]$.
arranged in order of decreasing eigenvalue $\lambda_i$.  We have shown above
that the best rank-$N$ linear approximation of $\myvec{x}$ is given by
\begin{equation}
  \myvec{x}^{(N)} = \sum_{i=1}^{N}a_i\myvec{v_i}.
\end{equation}
where the coefficients can be calulated as
\begin{equation}
  \label{eq:a_coeff_def}
  a_i = \myvec{v_i}^T\myvec{x}.
\end{equation}
When $\myvec{x}$ has missing data,
however, the question of how to compute $a_i$ is not as straightforward.
Simply setting the missing values to zero
will not work: the reconstruction will then faithfully recover those
zero values.  We desire instead to constrain the expected contribution
of each eigenvector to $x$ while {\it ignoring} the contribution of the
missing data.

A simple solution may be to simply truncate the vectors such that the dot
product is only computed over unmasked values.  It is easy to see that this
is identical to the  ``set to zero'' solution just
discussed.  Furthermore, there is the problem that in general, a set
of bases truncated in this way does not retain its orthogonality.

Another approach may be to derive a new basis which {\it is} orthogonal
over the truncated space.  This approach would be orthogonal and complete,
but the resulting coefficients could not easily be compared to coefficients
derived from objects observed with different masks.

We need a different approach.  We have shown above that when noise is
not present, the KL vectors define the optimal basis for rank-$N$
reconstruction in the least-squares sense.  That is, for an arbitrary
truncated orthonormal basis $\mymat{\Phi}_{(N)}$, (where truncated means
we use only the first $N$ columns of $\mymat{\Phi}$),
\begin{equation}
  \chi^2 = \left|\left(\myvec{x}
  - \mymat{\Phi}_{(N)}^T\myvec{a}^{(N)}\right)\right|^2
\end{equation}
is minimized on average when
$\mymat{\Phi} = \mymat{V}_{(N)}
= [\myvec{v_1}, \myvec{v_2} \cdots \myvec{v_M}]$, $N \le M$.
Here we flip the problem: we know the desired basis $\mymat{V}_{(N)}$,
and hope to find the optimal vector of coefficients $\myvec{a}^{(N)}$
which minimizes the $\chi^2$ in the presence of the missing data
\citep{Connolly99}.
We'll define a 
diagonal weight matrix $\mymat{W}$ such that $W_{ij} = w_i\delta_{ij}$,
where $w_i=1$ where $x_i$ is defined, and $w_i=0$ where $x_i$ is
missing.  Our expression to minimize then becomes
\begin{equation}
  \chi^2(\myvec{a}^{(N)}) = \left(\myvec{x}
  - \sum_{i=1}^N a^{(N)}_i \myvec{v^{(i)}}\right)^T
  \mymat{W}^2 \left(\myvec{x}
  - \sum_{i=1}^N a_{(N),i} \myvec{v^{(i)}}\right).
\end{equation}
To minimize this with respect to the coefficients $a_{(N),i}$, we differentiate
and find
\begin{equation}
  \frac{\partial\chi^2}{\partial a_{(N),i}} = -2 \myvec{x}^t\mymat{W}^2\myvec{v^{(i)}}
  + 2\sum_{j=1}^N a_{(N),j} \myvec{v^{(j)}}^T \mymat{W}^2 \mymat{v^{(i)}}.
\end{equation}
Setting the derivative to zero and combining terms gives
\begin{equation}
  \label{eq:masked_coeff}
  \myvec{x}^T\mymat{W}^2\myvec{v^{(i)}} =
  \sum_{j=1}^N a^{(N)}_j \myvec{v^{(j)}}^T \mymat{W}^2 \mymat{v^{(i)}}.
\end{equation}
If there are no areas of missing data, then $\mymat{W} = \mymat{I}$ and we
simply recover $a_i = \myvec{v_i}^T\myvec{x}$, our standard expression
for finding the KL coefficients with no missing data.
Because the inner-product of
$\myvec{v^{(i)}}$ and $\myvec{v^{(j)}}$ is modulated by $\mymat{W}$, however,
there is no delta function to collapse the sum.

We can simplify this notation by defining the correlation matrix of the
mask $\mymat{M}_{(N)}$, such that
\begin{equation}
  [\mymat{M}_N]_{ij} \equiv \myvec{v^{(j)}}^T \mymat{W}^2 \mymat{v^{(i)}}
\end{equation}
so that eq.~\ref{eq:masked_coeff} can be compactly written
\begin{equation}
  \myvec{x}^T \mymat{W}^2 \mymat{V}_{(N)}
  = {\myvec{a}^{(N)}}^T \mymat{M}_N.
\end{equation}
From this, we can quickly see that the optimal set of coefficients
$\myvec{a}^{(N)}$ is given by
\begin{equation}
  \label{eq:a_truncated}
  \myvec{a}^{(N)} = [\mymat{M}_N]^{-1}
  \mymat{V}_{(N)}^T \mymat{W}^2 \myvec{x}.
\end{equation}
This is the equivalent of the expression in eq.~\ref{eq:a_coeff_def}:
if $\mymat{W}$ is set equal to the identity matrix (indicating no
missing data), then $\mymat{M}_N$ is also the identity and we recover
eq.~\ref{eq:a_coeff_def} exactly.

If some of the diagonal entries in $\mymat{W}$ are zero, then the
correlation matrix for the full set of $K$ eigenvectors is rank-deficient,
and cannot be inverted as required for eq.~\ref{eq:a_truncated}.  For this
reason, it is essential to use the truncated eigenvectors $\mymat{V}_{(N)}$,
with $N \le rank(\mymat{W})$, the rank of the matrix $\mymat{W}$.  For the
case here where $\mymat{W}$ is a diagonal matrix consisting of zeros and
ones, the rank is equivalent to the trace, or the sum of nonzero diagonal
terms.

Once these approximate KL coefficients $\myvec{a}^{(N)}$ are determined,
it is straightforward to use these to approximate the unmasked vector
$\myvec{x}$:
\begin{equation}
  \myvec{x} \approx \sum_{i=0}^N a_i^{(N)} \myvec{v^{(i)}}.
\end{equation}
Here we have used the coefficients determined from the unmasked region of
the data to constrain the unobserved value in the masked regions.

This could be further generalized by allowing $\mymat{W}$ to be an arbitrary
matrix, for instance encoding the inverse of the noise covariance
associated with the observed vector $\myvec{x}$.  In unconstrained regions,
the noise is infinite and the inverse is zero.  This leads to very similar
results to those expressed here (in this case, $\mymat{W}^2$ is replaced
by $\mymat{W}^T\mymat{W}$).  This is equivalent to the whitening operation
discussed in \S\ref{sec:whitening}.  We will not use this formalism here, so we
leave it only as a suggested extension.

\rule{0.8\textwidth}{0.5pt}

{\bf Break Here.  The remainder of this chapter is copied from one of
  my papers}

\section{\KL\ Analysis of Shear}
\label{KL_Intro}
Any set of $N$-dimensional data can be represented as a sum of 
$N$ orthogonal basis functions: this amounts to a rotation and scaling of 
the $N$-dimensional coordinate axis spanning the space in which the data live.
KL analysis seeks a set of orthonormal basis functions which can optimally
represent the dataset.  The sense in which the KL basis is optimal will be
discussed below.  For the current work, the data we wish to represent are the 
observed gravitational shear measurements across the sky.  
We will divide the survey 
area into $N$ discrete cells, at locations $\myvec{x}_i,\ 1\le i \le N$.  
From the ellipticity of the galaxies within each cell, 
we infer the observed shear $\gamma^o(\myvec{x}_i)$, which we assume
to be a linear combination of the true underlying shear, $\gamma(\myvec{x}_i)$
and the shape noise $n_\gamma(\myvec{x}_i)$.\footnote{
Throughout this work, we assume we are in the regime where the convergence
$\kappa \ll 1$ so that the average observed ellipticity in a 
cell is an unbiased estimator of shear; see \citet{Bartelmann01}}
In general, the cells may be of any shape (even overlapping) 
and may also take into account the redshift of sources.
In this analysis, the cells will be square pixels across the locally 
flat shear field, with no use of source redshift information.  
For notational clarity, we will represent quantities with a vector notation,
denoted by bold face: i.e. $\myvec{\gamma} = [\gamma_1,\gamma_2\cdots]^T$; 
$\gamma_i = \gamma(\myvec{x}_i)$. 

\subsection{KL Formalism}
\label{KL_Formalism}
KL analysis provides a framework such that our measurements $\myvec\gamma$ 
can be expanded in a set of $N$ orthonormal basis functions 
$\left\{ \myvec{\Psi}_j(\myvec{x}_i),\ j=1,N\right\}$, via a vector of
coefficients $\myvec{a}$.  In matrix form, the relation can be written
\begin{equation}
  \myvec\gamma = \myvec\Psi\myvec{a}
\end{equation}
where the columns of the matrix $\myvec\Psi$ are the basis vectors 
$\myvec\Psi_i$.  Orthonormality is given by the condition 
$\myvec\Psi_i^\dagger\myvec\Psi_j = \delta_{ij}$, so that the coefficients
can be determined by
\begin{equation}
  \myvec{a} = \myvec{\Psi}^\dagger\myvec{\gamma}
\end{equation}
A KL decomposition is optimal in the sense that it seeks basis 
functions for which the 
coefficients are statistically orthogonal;\footnote{Note that statistical
orthogonality of coefficients is conceptually distinct from the 
geometric orthogonality of the basis functions themselves; 
see \citet{Vogeley96} for a discussion of this property.}
that is, they satisfy
\begin{equation}
  \langle a_i^* a_j \rangle = \langle a_i^2 \rangle \delta_{ij}
\end{equation}
where angled braces $\langle\cdots\rangle$ denote averaging over all 
realizations.  This definition leads to several important properties
\citep[see][for a thorough discussion \& derivation]{Vogeley96}:
\begin{enumerate}
\item \textbf{KL as an Eigenvalue Problem:} 
  Defining the correlation matrix 
  $\myvec{\xi}_{ij} = \langle \gamma_i\gamma_j^*\rangle$, 
  it can be shown that the KL vectors $\myvec{\Psi}_i$ are eigenvectors 
  of $\myvec{\xi}$ with eigenvalues $\lambda_i = \langle a_i^2\rangle$.
  For clarity, we'll order the eigenbasis such that 
  $\lambda_i \ge \lambda_{i+1}\ \forall\ i\in(1,N-1)$.  We define the
  diagonal matrix of eigenvalues $\mymat{\Lambda}$, such that
  $\mymat{\Lambda}_{ij} = \lambda_i\delta_{ij}$
  and write the eigenvalue decomposition in compact form:
  \begin{equation}
    \mymat{\xi} = \mymat{\Psi}\mymat{\Lambda}\mymat{\Psi}^\dagger
  \end{equation}

\item \textbf{KL as a Ranking of Signal-to-Noise}
  It can be shown that KL vectors of a whitened covariance matrix (see
  Section~\ref{Adding_Noise})
  diagonalize both the signal and the noise of the problem, with the
  signal-to-noise ratio proportional to the eigenvalue.  This is
  why KL modes are often called ``Signal-to-noise eigenmodes''.

\item \textbf{KL as an Optimal Low-dimensional Representation:}
  An important consequence of the signal-to-noise properties of KL modes  
  is that the optimal rank-$n$ representation of the data is 
  contained in the KL vectors corresponding to the $n$ largest eigenvalues:
  that is,
  \begin{equation}
    \label{eq_truncation}
    \myvec{\hat\gamma}^{(n)}
    \equiv \sum_{i=1}^{n<N} a_i\myvec{\Psi}_i
  \end{equation}
  minimizes the reconstruction error between $\myvec{\hat\gamma}^{(n)}$ and 
  $\myvec\gamma$ for reconstructions using $n$ orthogonal basis vectors.
  This is the theoretical basis of Principal Component Analysis (sometimes
  called Discrete KL), and leads to a common application of KL 
  decomposition: filtration of noisy signals.  For notational compactness,
  we will define the truncated eigenbasis $\mymat{\Psi}_{(n)}$ and truncated
  vector of coefficients $\myvec{a}_{(n)}$ such that 
  Equation~\ref{eq_truncation} can be written in matrix form:
  $\myvec{\hat\gamma}^{(n)} = \mymat{\Psi}_{(n)}\myvec{a}_{(n)}$.
\end{enumerate}
 
\subsection{KL in the Presence of Noise}
\label{Adding_Noise}
When noise is present in the data, the above properties do not 
necessarily hold.
To satisfy the statistical orthogonality of the KL coefficients $\myvec{a}$ 
and the resulting signal-to-noise properties of the KL eigenmodes, 
it is essential that the noise in the covariance matrix be ``white'': 
that is, $\Noise_\gamma \equiv 
\langle \myvec{n}_\gamma\myvec{n}_\gamma^\dagger \rangle \propto \mymat{I}$.  
This can be accomplished through a judicious
choice of binning, or by rescaling the covariance with a whitening 
transformation.  We take the latter approach here.

Defining the noise covariance matrix $\Noise_\gamma$ as above,
the whitened covariance matrix can be written 
$\myvec{\xi}_W = \Noise_\gamma^{-1/2} \myvec{\xi} \Noise_\gamma^{-1/2}$.  Then 
the whitened KL modes become
$\myvec{\Psi}_W\myvec{\Lambda}_W\myvec{\Psi}_W^\dagger \equiv \myvec{\xi}_W$.
The coefficients $\myvec{a}_W$ are calculated from the noise-weighted signal,
that is
\begin{equation}
  \myvec{a}_W = \myvec{\Psi}_W^\dagger\Noise_\gamma^{-1/2}
  (\myvec{\gamma}+\myvec{n}_\gamma)
\end{equation}
For the whitened KL modes, if signal and noise are uncorrelated, this leads to 
$\langle\myvec{a}_W\myvec{a}_W^\dagger\rangle = \myvec{\Lambda}_W + \myvec{I}$:
that is, the coefficients $\myvec{a}_W$ are statistically orthogonal.
For the remainder of this work, we will drop the subscript ``$_W$'' and assume
all quantities to be those associated with the whitened covariance.



\section{KL for Parameter Estimation}
\label{sec:kl_intro}
KL analysis and the related Principal Component Analysis are well-known
statistical tools which have been applied in a wide variety of astrophysical
situations, from e.g. analysis of the spatial power of galaxy counts
\citep{Vogeley96, Szalay03, Pope04}
to characterization of stellar, galaxy, and QSO spectra
\citep{Connolly95, Connolly99, Yip04a, Yip04b},
to studies of noise properties of weak lensing surveys
\citep{Kilbinger06, Munshi06}, and a host of other situations too numerous
to mention here.  Informally, the power of KL/PCA rests in the fact that 
it allows a highly efficient representation of a set of data, highlighting
the components which are most important in the dataset as a whole.
The discussion of KL analysis below derives largely from \citet{Vogeley96},
reexpressed for application in cosmic shear surveys.

Any $D$-dimensional data point may be represented as a linear combination of 
$D$ orthogonal basis functions.  
For example, the data may be $N$ individual galaxy spectra, each with flux
measurements in $D$ wavelength bins.  Each spectrum can be thought of as a
single point in $D$-dimensional parameter space, where each axis corresponds
to the value within a single wavelength bin.  
Geometrically, there is nothing special about
this choice of axes: one could just as easily rotate and translate the axes
to obtain a different but equivalent representation of the same data.

In the case of of a shear survey, our single data vector is the set of
cosmic shear measurements across the sky.  We will divide the sky into $N$
cells in angular and redshift space, at coordinates
$\myvec{x}_i = (\theta_{x,i}, \theta_{y,i}, z_i)$
These cells may be spatially distinct, or they may overlap.
From the ellipticity of the galaxies within each cell, we
estimate the shear
$\gamma_i \equiv \gamma^o(\myvec{x}_i) = 
\gamma(\myvec{x}_i) + n_\gamma(\myvec{x}_i)$
where $\gamma(\myvec{x}_i)$ is the true underlying shear,
and $n_\gamma(\myvec{x}_i)$ is the measurement noise.
Our data vector is then
$\myvec{\gamma} = [\gamma_1, \gamma_2 \cdots \gamma_N]^T$.

We seek to express our set of measurements $\myvec{\gamma}$
as a linear combination of $N$ (possibly complex) 
orthonormal basis vectors
$\{\myvec{\Psi}_j(\myvec{x}_i, j=1,N)\}$ with complex coefficients
$a_j$:
\begin{equation}
  \label{eq:gamma_decomp}
  \gamma_i = \sum_{j=1}^{N} a_j \Psi_j(\myvec{x}_i)
\end{equation}
For conciseness, we'll create the matrix $\mymat{\Psi}$ whose columns are
the basis vectors $\myvec{\Psi}_j$, so that the above equation can be
compactly written $\myvec\gamma = \mymat\Psi\myvec{a}$.  Orthonormality
of the basis vectors leads to the property
$\mymat\Psi^\dagger\mymat\Psi = \mymat{I}$, where $\mymat{I}$ is the identity
matrix: that is, $\mymat\Psi$ is a unitary matrix with
$\mymat\Psi^{-1} = \mymat\Psi^\dagger$.  Observing this, we can easily compute
the coefficients for a particular data vector:
\begin{equation}
  \myvec{a} = \mymat\Psi^\dagger \myvec\gamma.
\end{equation}
We will be testing the likelihood of a particular set of coefficients
$\myvec{a}$.  
The statistical properties of these coefficients can be written in terms of
the covariance of the observed shear:
\begin{equation}
  \label{eq:a_cov}
  \left\langle \myvec{a}\myvec{a}^\dagger \right\rangle 
  =  \mymat\Psi^\dagger
  \left\langle \myvec\gamma\myvec\gamma^\dagger \right\rangle 
  \mymat\Psi
  \equiv \mymat\Psi^\dagger \myvec{\xi}  \mymat\Psi
\end{equation}
where we have defined the observed shear correlation matrix 
$\myvec{\xi} \equiv \left\langle 
\myvec\gamma\myvec\gamma^\dagger \right\rangle$, and angled braces
$\langle\cdots\rangle$ denote expectation value or ensemble average
of a quantity.

Because we hope to perform a likelihood analysis on the coefficients
$\myvec{a}$, it will be useful in likelihood estimation if they are
statistically orthogonal:
\begin{equation}
  \label{eq:a_cov_2}
  \left\langle \myvec{a}\myvec{a}^\dagger \right\rangle_{ij}
  = \left\langle a_i^2 \right\rangle \delta_{ij}
\end{equation}
Comparing Equations \ref{eq:a_cov} \& \ref{eq:a_cov_2} we see that the desired
basis functions are the solution of the eigenvalue problem
\begin{equation}
  \mymat\xi \myvec\Psi_j = \lambda_j \myvec\Psi_j
\end{equation}
where the eigenvalue $\lambda_j = \left\langle a_i^2 \right\rangle$.
By convention, we'll order the eigenvalue/eigenvector pairs such that
$\lambda_i \ge \lambda_{i+1} \forall i\in(1, N-1)$.
Expansion of the data $\myvec\gamma$ into this basis is the discrete form
of KL analysis.

A KL decomposition has a number of useful properties:
\begin{description}
  \item[Uniqueness] A KL decomposition is a unique representation of the data.
    That is, there only a single set of basis vectors which satisfy the above
    properties (up to degeneracies resulting from identical eigenvalues)
    This can be straightforwardly shown in a proof by contradiction
    \citep[e.g.][]{Vogeley96}.
  \item[Efficiency] A partial KL decomposition provides the
    optimal low-rank approximation of an observed data vector.  That is,
    for $n < N$, the partial reconstruction (cf. Eqn.~\ref{eq:gamma_decomp})
    \begin{equation}
      \myvec\gamma^{(n)} \equiv \sum_{j=1}^n a_j \Psi_j
    \end{equation}
    minimizes the average reconstruction error
    $\epsilon_n \equiv |\myvec\gamma - \myvec\gamma^{(n)}|^2$
    for any orthogonal basis set $\mymat\Psi$.  The proof can be easily
    obtained using Lagrangian multipliers \citep[again, see][]{Vogeley96}.
  \item[Signal-to-noise Optimization] As a consequence of the efficiency
    property, it is clear that for data with white noise\footnote{
    By \textit{white noise} we mean that the noise covariance satisfies
    $\mymat{\mathcal{N}}_{ij} \equiv 
    \langle\myvec{n_\gamma}\myvec{n_\gamma}^\dagger\rangle_{ij} 
    \propto \delta_{ij}$}, KL modes provide the
    maximum possible signal-to-noise ratio per mode.  The noise can be assured
    to be white through a judicious choice of binning, or alternatively
    the data can be artificially whitened (See \S\ref{sec:whitening}).
    If signal and noise are uncorrelated, then the covariance of the observed
    shear can be decomposed as
    \begin{equation}
      \mymat{\xi} = \mymat{\mathcal{S}} + \mymat{\mathcal{N}}
    \end{equation}
    Because the noise covariance $\mymat{\mathcal{N}} \equiv 
    \langle\myvec{n_\gamma}\myvec{n_\gamma}^\dagger\rangle$ is proportional
    to the identity by assumption, Diagonalization of $\mymat{\xi}$ results
    in a simultaneous diagonalization of both the signal $\mymat{\mathcal{S}}$
    and the noise $\mymat{\mathcal{N}}$.  Because of this signal-to-noise
    optimization property, KL modes can be proven to be the optimal basis
    for testing of spatial correlations \citep[see Appendix A of][]{Vogeley96}.
\end{description}

\subsection{Shear Noise Properties}
\label{sec:whitening}
The signal-to-noise properties of shear mentioned above are based on the 
requirement that noise be ``white'', that is, the noise covariance is
$\mymat{\mathcal{N}} \equiv 
\langle\myvec{n_\gamma}\myvec{n_\gamma}^\dagger\rangle
= \sigma^2 \mymat{I}$.  Noise in measured shear is affected mainly by the
intrinsic ellipticity and source density, but can also be prone to systematic
effects which lead to noise correlations between pixels.  When the survey
geometry leads to shear with more complicated noise characteristics, a
whitening transformation can be applied.

Given the measured data $\myvec\gamma$ and noise covariance
$\mymat{\mathcal{N}}$, we can define the whitened shear
\begin{equation}
  \myvec{\gamma}^\prime = \mymat{\mathcal{N}}^{-1/2} \myvec{\gamma}
\end{equation}
With this definition, the shear covariance matrix becomes
\begin{eqnarray}
  \mymat{\xi}^\prime 
  &=& \left\langle \myvec{\gamma}^\prime 
  \myvec{\gamma}^{\prime\dagger}\right\rangle \nonumber\\
  &=& \mymat{\mathcal{N}}^{-1/2}\mymat{\xi}
  \mymat{\mathcal{N}}^{-1/2} \nonumber\\
  &=& \mymat{\mathcal{N}}^{-1/2}\left[
    \mymat{\mathcal{S}} + \mymat{\mathcal{N}}
    \right]\mymat{\mathcal{N}}^{-1/2} \nonumber\\
  &=& \mymat{\mathcal{N}}^{-1/2}\mymat{\mathcal{S}}\mymat{\mathcal{N}}^{-1/2} + \mymat{I}
\end{eqnarray}
We see that the whitened signal is $\mymat{\mathcal{S}}^\prime = 
\mymat{\mathcal{N}}^{-1/2}\mymat{\mathcal{S}}\mymat{\mathcal{N}}^{-1/2}$
and the whitened noise is $\mymat{\mathcal{N}}^\prime = \mymat{I}$, the
identity matrix. So this transformation in fact whitens the data covariance,
so that the noise in each bin is constant and uncorrelated.  Given the
whitened measurement covariance $\mymat{\xi}^\prime$, we can find the KL
decomposition which satisfies the eigenvalue problem
\begin{equation}
  \mymat{\xi}^\prime \myvec{\Psi^\prime}_j = 
  \lambda^\prime_j \myvec{\Psi^\prime}_j
\end{equation}
With KL coefficients given by
\begin{equation}
  \myvec{a}^\prime = \mymat{\Psi}^{\prime\dagger}
  \mymat{\mathcal{N}}^{-1/2}\myvec\gamma
\end{equation}
Note that because $\langle\myvec\gamma\rangle = 0$,
the expectation value of the KL coefficients is
\begin{eqnarray}
  \langle\myvec{a}^\prime\rangle 
  &=& \mymat{\mathcal{N}}^{-1/2}\langle\myvec\gamma\rangle\nonumber\\
  &=& 0
\end{eqnarray}
For the remainder of this work, it will be assumed that we are working with
whitened quantities.  The primes will be dropped for notational simplicity.

\subsection{Constructing the Covariance Matrix}
In many applications, the data covariance matrix can be estimated
empirically, using the fact that
\begin{equation}
  \tilde{\myvec\xi} = \lim_{N\to\infty} \sum_{i=1}^N 
  \myvec{\gamma}_i \myvec{\gamma}_i^\dagger
\end{equation}
Unfortunately, in surveys of cosmic shear, we have only a single sky to
observe, so this approach does not work.  Instead, we can construct the
measurement covariance analytically by assuming a theoretical form of the
underlying matter power spectrum.

The measurement covariance $\mymat{\xi}_{ij}$ between two regions of the
sky $A_i$ and $A_j$ is given by
\begin{eqnarray}
  \label{eq:xi_analytic}
  \myvec{\xi}_{ij} 
  &=& \mymat{\mathcal{S}}_{ij} + \mymat{\mathcal{N}}_{ij} \nonumber\\
  &=& \left[\int_{A_i}d^2x_i\int_{A_j}d^2x_j 
    \xi_+(|\myvec{x_i}-\myvec{x_j}|)\right]
  + \mymat{\mathcal{N}}_{ij}
\end{eqnarray}
where $\xi_+(\theta)$ is the ``+'' shear correlation function. 
$\xi_+(\theta)$ is expressible as an integral over the shear power spectrum
weighted by the zeroth-order Bessel function
\citep[see, e.g.][]{Schneider02}:
\begin{equation}
  \label{eq:xi_plus_def}
  \xi_+(\theta) 
  = \frac{1}{2\pi} \int_0^\infty d\ell\ \ell P_\gamma(\ell) J_0(\ell\theta)
\end{equation}
The angular shear power spectrum $P_\gamma(\ell)$ can be expressed as a
weighted line-of-sight integral over the matter power 
spectrum \citep[see, e.g.][]{Takada04}:
\begin{equation}
  \label{eq:P_gamma}
  P_\gamma(\ell) = \int_0^{\chi_s}d\chi W^2(\chi)\chi^{-2}
  P_\delta\left(k=\frac{\ell}{\chi};z(\chi)\right)
\end{equation}
Here $\chi$ is the comoving distance, $\chi_s$ is the distance to the
source, and $W(\chi)$ is the lensing weight function,
\begin{equation}
  \label{eq:lensing_weight}
  W(\chi) = \frac{3\Omega_{m,0}H_0^2}{2a(\chi)}\frac{\chi}{\bar{n}_g}
  \int_{\chi}^{\chi_s}dz\ n(z) \frac{\chi(z)-\chi}{\chi(z)}
\end{equation}
where $n(z)$ is the empirical redshift distribution of galaxies.
The nonlinear mass fluctuation power spectrum $P_\delta(k, z)$ can be
predicted semianalytically: in this work we use the halo model of
\citet{Smith03}.  With this as an input, we can analytically
construct the measurement covariance matrix $\mymat\xi$ using 
Equations~\ref{eq:xi_analytic}-\ref{eq:lensing_weight}.

\subsection{Cosmological Likelihood Analysis with KL}
From the survey geometry and galaxy ellipticities, we measure the
shear $\myvec\gamma$, estimate the noise covariance
$\mymat{\mathcal{N}}$ (see \S\ref{sec:bootstrap}) and derive
the whitened covariance matrix $\mymat\xi$. 
From $\mymat\xi$ we compute the KL basis $\mymat\Psi$ and $\myvec\lambda$.
Using the KL basis, we compute the coefficients
$\myvec{a} = \mymat{\Psi}^\dagger \mymat{\mathcal{N}}^{-1/2} \myvec\gamma$.
Given these KL coefficients $\myvec{a}$, we use a Bayesian framework to
compute the posterior distribution of our cosmological parameters.

Given observations $D$ and prior information $I$, Bayes' theorem specifies the
posterior probability of a model described by the parameters $\{\theta_i\}$:
\begin{equation}
  \label{eq:bayes}
  P(\{\theta_i\}|DI) = P(\{\theta_i\}|I) \frac{P(D|\{\theta_i\}I)}{P(D|I)}
\end{equation}
The term on the LHS is the \textit{posterior} probability of the set of
model parameters $\{\theta_i\}$, which is the quantity we are interested in.

The first term on the RHS is the \textit{prior}.  It quantifies how our prior
information $I$ affects the probabilities of the model parameters.  The 
prior is where information from other surveys (e.g. WMAP, etc) can be
included. The likelihood function for the observed coefficients $\myvec{a}$
enters into the numerator $P(D|\{\theta_i\}I)$.  The denominator $P(D|I)$
is essentially a normalization constant, set so that the sum of probabilities
over the parameter space equals unity.

For a given model $\{\theta_i\}$, we can predict the expected distribution of model KL
coefficients $\myvec{a}_{\{\theta_i\}} \equiv \mymat{\Psi}^\dagger
\mymat{\mathcal{N}}^{-1/2}\myvec{\gamma}$:
\begin{eqnarray}
  \mymat{C}_{\{\theta_i\}}
  & \equiv & \langle\myvec{a}_{\{\theta_i\}}
  \myvec{a}_{\{\theta_i\}}^\dagger\rangle\nonumber\\
  &=& \mymat{\Psi}^\dagger \mymat{\mathcal{N}}^{-1/2} 
  \mymat{\xi}_{\{\theta_i\}}\mymat{\mathcal{N}}^{-1/2}\mymat{\Psi}
\end{eqnarray}
Using this, the measure of departure from the model $m$ is given by the
quadratic form
\begin{equation}
  \chi^2 = \myvec{a}^\dagger\mymat{C}_{\{\theta_i\}}^{-1}\myvec{a}
\end{equation}
The likelihood is then given by
\begin{equation}
  \label{eq:likelihood}
  \mathcal{L}(\myvec{a}|\{\theta_i\}) = 
  (2\pi)^{n/2} |\det(C_{\{\theta_i\}})|^{-1/2}
  \exp(-\chi^2/2)
\end{equation}
where $n$ is the number of degrees of freedom: that is, the number
of eigenmodes included in the analysis.  The likelihood given by
Equation~\ref{eq:likelihood} enters into Equation~\ref{eq:bayes} when
computing the posterior probability.


