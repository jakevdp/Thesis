\chapter{Introduction to \KL\ Analysis}

%\begin{itemize}
%  \item Introduce the notational formalism
%  \item KL as an eigenvalue problem
%  \item KL as a signal-to-noise ranking
%  \item KL as an optimal low-dimensional representation
%  \item KL on noisy data
%  \item KL with missing information
%\end{itemize}

%Also mention LLE and cite \cite{Vanderplas2009}, which uses a nonlinear
%alternative to PCA for dimensionalty reduction.

\KL\ (KL) analysis is a commonly used statistical tool
in a broad range of astronomical applications, from, e.g.~studies of 
galaxy and quasar spectra \citep{Connolly95,Connolly99,Yip04a,Yip04b}, to 
analysis of the spatial distribution of galaxies 
\citep{Vogeley96,Matsubara00,Pope04}, to characterization of the 
expected errors in weak lensing surveys \citep{Kilbinger06, Munshi06}.
In this chapter, we will develop the formalism of KL which will form
the basis of the results in the subsequent chapters.

The KL formalism requires the liberal
employment of algebra with vectors, scalars, and matrices.
For clarity, we will begin by briefly specifying the notational
conventions used in this chapter and throughout this work.

\section{Notational Conventions}

It is important to clearly distinguish between vectors, matrices, and
scalars in the following formulation.  Vectors will be denoted by
bold lower-case symbols; e.g. $\myvec{x}$.  Matrices will be denoted by
bold upper-case symbols; e.g. $\mymat{X}$.  Scalars will be denoted by
non-bold symbols, either upper or lower-case.
All vectors are assumed to be column vectors, while a row-vector is
indicated by the transpose, $\myvec{x}^T$.  
Single elements of a given
vector or matrix are given with subscripts: $x_i$ is the $i^{\rm th}$
element of the vector $\myvec{x}$, and $X_{ij}$ is the element in the
$i^{\rm th}$ row and $j^{\rm th}$ column of the matrix $\mymat{X}$.
The column vector making up the $j^{\rm th}$ row of $\mymat{X}$ is
indicated by $\myvec{x}^{(j)}$.  Note then, that by this convention,
the $(i, j)$ component of matrix $\mymat{X}$ can be equivalently expressed
$X_{ij}$ or $x_i^{(j)}$.

In algebraic expressions, the normal linear algebra rules are assumed.
For example, the expression 
\begin{equation}
  \myvec{y} = \mymat{M}\mymat{x} + \mymat{b}
\end{equation}
involves the vectors $\myvec{y}$, $\myvec{x}$, and $\myvec{b}$ and the
matrix $\mymat{M}$.  This expression is short-hand for the summation:
\begin{equation}
  y_i = \sum_{j=1}^{n} M_{ij} x_j + b_i
\end{equation}
Using these rules, we can define the magnitude of a vector
\begin{equation}
  |\myvec{x}| \equiv \sqrt{\myvec{x}^T \myvec{x}}
\end{equation}

\section{Basis function decomposition}
KL analysis is simply a basis function decomposition, where the basis
functions are derived based on the variance and covariance properties
of a class of functions.  We'll start by describing what is perhaps the
best-known basis function decomposition, the Fourier Transform.
We start here partly because it offers a generalization to the fundamental
ideas of KL analysis, and partly because it becomes very important in
cosmology when studying correlation functions and power spectra.

\subsection{Fourier Transforms}
The Fourier Transform is a means of expressing a function in terms
of a certain class of oscillatory basis functions.
Given an arbitrary continuous function
$f(t)$, it can be re-expressed in terms of sinusoidal basis functions
$\Phi_k(t) \equiv e^{ikt} / \sqrt{2\pi}$
\begin{equation}
  \label{eq:FT_1D}
  f(t) = \int_{-\infty}^\infty \hat{f}_k \Phi_k(t) \dd k.
\end{equation}
What is more, it can be shown that this {\it Fourier Transform} $\hat{f}_k$
is uniquely determined for a given $f(t)$.  But what does this integral mean?
It specifies a set of basis functions $\Phi_k(t)$ and an associated set of
coefficients $f_k$, such that the function $f(t)$ can be reconstructed
from the basis functions.  Because of the uniqueness of $f_k$ for a given
$f(t)$, $f_k$ is in some sense a replacement for $f(t)$.  We can compute
the Fourier transform $f_k$ and forget the originial information $f(t)$,
being confident that $f(t)$ can be perfectly reconstructed from the
information on $f_k$.

But how can we compute the Fourier Transform $f_k$ given $f(t)$?  Though
the expression is well-known, we'll briefly derive it here because it
illuminates some of the properties of Fourier transforms which will
generalize to KL tranforms.

To begin, we'll multiply both sides of Equation~\ref{eq:FT_1D} by the
basis function $\Phi^*_{k^\prime}(t)$, and integrate over all $t$.
Here the $\ast$ denotes the complex conjugate.  This gives
\begin{equation}
  \label{eq:FT_1D}
  \int_{-\infty}^\infty \dd t \Phi^\ast_{k^\prime}(t) f(t)
  = \int_{-\infty}^\infty \dd t \Phi^\ast_{k^\prime}(t)
  \int_{-\infty}^\infty \dd k \hat{f}_k \Phi_k(t).
\end{equation}
Switching the order of integration and rearranging terms, we find
\begin{equation}
  \label{eq:IFT_deriv1}
  \int_{-\infty}^\infty \dd t \Phi^\ast_{k^\prime}(t) f(t)
  = \int_{-\infty}^\infty \dd k \hat{f}_k
  \int_{-\infty}^\infty \dd t \Phi^\ast_{k^\prime}(t)\Phi_k(t)
\end{equation}
Examining the integral over $t$ on the right-hand side of the equation,
we find
\begin{eqnarray}
  \int_{-\infty}^\infty \dd t \Phi^\ast_{k^\prime}(t)\Phi_k(t)
  &=& \frac{1}{2\pi} \int_{-\infty}^\infty
  \dd t e^{i t (k - k^\prime)}\nonumber\\
  &=& \delta(k - k^\prime)
\end{eqnarray}
where in the last line we have used the definition of the dirac delta
function,
\begin{equation}
  \delta(k) \equiv \frac{1}{2\pi} \int_{-\infty}^\infty e^{ikt} \dd k.
\end{equation}
Putting this delta function into the integral in Equation~\ref{eq:IFT_deriv1}
and collapsing the integral gives
\begin{equation}
  f_{k^\prime} = \int_{-\infty}^\infty \Phi^\ast_{k^\prime}(t) f(t) \dd t.
\end{equation}
Substituting $k^\prime \to k$ yields the familiar expression
\begin{equation}
  \label{eq:IFT_1D}
  f_k = \int_{-\infty}^\infty f(t) e^{-ikt} \dd t.
\end{equation}

Equation~\ref{eq:IFT_1D} shows how to compute the Fourier transform
$f_k$ for a given $f(t)$.  But one might wonder if this is a unique
result.  Could there be many different valid Fourier transforms for a given
continuous function?

Let's assume that given a function $f(t)$, there are two valid Fourier
transforms, given by $f_k$ and $f^\prime_k$.  In this case, from
Equation~\ref{eq:FT_1D} we have
\begin{equation}
  f(t) - f(t) = \int_{-\infty}^\infty (f_k - f^\prime_k) \Phi_k(t) \dd k = 0
\end{equation}
Similarly to above, we can multiply by $\Phi^\ast_{k^\prime}(t)$,
integrate over all $t$, and extract a delta function to yield:
\begin{equation}
  \int_{-\infty}^\infty (f_k - f^\prime_k) \delta(k - k^\prime) \dd k = 0.
\end{equation}
Collapsing the integral we find that $f_k = f^\prime_k$ must hold for all $k$;
that is, the Fourier transform for continuously integrable functions
$f(t)$ is unique\footnote{The uniqueness property does not necessarily
hold for arbitrary functions $f(t)$ which are not piecewise continuous
or square-integrable.}

Above, we have shown the case of continuous Fourier Transforms on an
unbounded domain $x \in (-\infty, \infty)$.  Very similar results can
be derived on bounded domains, i.e.~$x \in (a, b)$.  The primary difference
is that the integral over $k$ becomes a sum over $k$, and the basis functions
are given by
\begin{equation}
  \Phi_k(x) = \exp\left[\frac{2\pi i k (x - a)}{b - a}\right]
\end{equation}
with a Fourier series expansion
\begin{equation}
  \label{eq:FS_1D}
  f_{(a, b)}(x) = \sum_{k=-\infty}^\infty f_k \Phi_k(x)
\end{equation}
and coefficients given by
\begin{equation}
  \label{eq:IFS_1D}
  f_k = \frac{1}{b - a}\int_a^b f_{(a, b)}(x) \Phi^\ast_k(x) \dd x
\end{equation}

\subsection{Generalizing Orthonormal Bases}

Stepping back for a moment, we have shown that for a particular
class of functions $\Phi_k(t)$, one can find unique coefficients
$f_k$ such that one of the expansions of
Equations~\ref{eq:FT_1D} or \ref{eq:FS_1D} hold.
A key observation to make is that all the derivations above rested 
soley on the orthonomality property of the basis functions: that is,
$\Phi_k(t)$ satisfies
\begin{equation}
  \int_a^b \Phi_k(t) \Phi^\star_{k^\prime}(t) 
  = \delta(k - k^\prime).
\end{equation}
Aside from this, nothing more is required to repeat the above derivations.
This suggests the possible existence of other functions which fit these
criteria.  Some examples are the Legendre polynomials on the interval
$[-1, 1]$, the Laguerre polynomials on the interval $[0, \infty)$, and
the Hermite polynomials on the interval $(\infty, \infty)$.
Just as there are an infinite number of possibile geometric
bases in a vector space, there are an infinite number of possible orthogonal
function bases that work in the above formalism.  Choosing the right basis
can lead to a much easier analysis of a given problem.

\section{\KL\ Analysis}
Because of the infinite number of possible orthogonal function classes,
one might wonder how to choose the optimal class for any particular problem.
\KL\ Analysis Seeks to answer this question in a very general case.

Imagine now that we have a centered random process $F_t$.  By {\it random
process}, we mean that there are multiple (potentially infinite)
realizations of this function at each value $t$.  By {\it centered},
we mean that the expectation value $E[F_t] = 0$ for all $t$.  Furthermore,
our random variables $F_t$ can be arbitrarily correlated: that is, the value
drawn at one $t$ can potentially depend on every other value drawn in that
particular realization.

The \KL\ theorem states that for a centered random process $F_t$ with a
continuous covariance $C_F(t, t^\prime)$, the eigenfunctions $e_k(t)$
of $C_F(t, t^\prime)$ are an orthonormal basis for $F_t$,
\begin{equation}
  F_t = \sum_{k = 1}^\infty
\end{equation}







\section{\KL\ Analysis of Shear}
\label{KL_Intro}
Any set of $N$-dimensional data can be represented as a sum of 
$N$ orthogonal basis functions: this amounts to a rotation and scaling of 
the $N$-dimensional coordinate axis spanning the space in which the data live.
KL analysis seeks a set of orthonormal basis functions which can optimally
represent the dataset.  The sense in which the KL basis is optimal will be
discussed below.  For the current work, the data we wish to represent are the 
observed gravitational shear measurements across the sky.  
We will divide the survey 
area into $N$ discrete cells, at locations $\myvec{x}_i,\ 1\le i \le N$.  
From the ellipticity of the galaxies within each cell, 
we infer the observed shear $\gamma^o(\myvec{x}_i)$, which we assume
to be a linear combination of the true underlying shear, $\gamma(\myvec{x}_i)$
and the shape noise $n_\gamma(\myvec{x}_i)$.\footnote{
Throughout this work, we assume we are in the regime where the convergence
$\kappa \ll 1$ so that the average observed ellipticity in a 
cell is an unbiased estimator of shear; see \citet{Bartelmann01}}
In general, the cells may be of any shape (even overlapping) 
and may also take into account the redshift of sources.
In this analysis, the cells will be square pixels across the locally 
flat shear field, with no use of source redshift information.  
For notational clarity, we will represent quantities with a vector notation,
denoted by bold face: i.e. $\myvec{\gamma} = [\gamma_1,\gamma_2\cdots]^T$; 
$\gamma_i = \gamma(\myvec{x}_i)$. 

\subsection{KL Formalism}
\label{KL_Formalism}
KL analysis provides a framework such that our measurements $\myvec\gamma$ 
can be expanded in a set of $N$ orthonormal basis functions 
$\left\{ \myvec{\Psi}_j(\myvec{x}_i),\ j=1,N\right\}$, via a vector of
coefficients $\myvec{a}$.  In matrix form, the relation can be written
\begin{equation}
  \myvec\gamma = \myvec\Psi\myvec{a}
\end{equation}
where the columns of the matrix $\myvec\Psi$ are the basis vectors 
$\myvec\Psi_i$.  Orthonormality is given by the condition 
$\myvec\Psi_i^\dagger\myvec\Psi_j = \delta_{ij}$, so that the coefficients
can be determined by
\begin{equation}
  \myvec{a} = \myvec{\Psi}^\dagger\myvec{\gamma}
\end{equation}
A KL decomposition is optimal in the sense that it seeks basis 
functions for which the 
coefficients are statistically orthogonal;\footnote{Note that statistical
orthogonality of coefficients is conceptually distinct from the 
geometric orthogonality of the basis functions themselves; 
see \citet{Vogeley96} for a discussion of this property.}
that is, they satisfy
\begin{equation}
  \langle a_i^* a_j \rangle = \langle a_i^2 \rangle \delta_{ij}
\end{equation}
where angled braces $\langle\cdots\rangle$ denote averaging over all 
realizations.  This definition leads to several important properties
\citep[see][for a thorough discussion \& derivation]{Vogeley96}:
\begin{enumerate}
\item \textbf{KL as an Eigenvalue Problem:} 
  Defining the correlation matrix 
  $\myvec{\xi}_{ij} = \langle \gamma_i\gamma_j^*\rangle$, 
  it can be shown that the KL vectors $\myvec{\Psi}_i$ are eigenvectors 
  of $\myvec{\xi}$ with eigenvalues $\lambda_i = \langle a_i^2\rangle$.
  For clarity, we'll order the eigenbasis such that 
  $\lambda_i \ge \lambda_{i+1}\ \forall\ i\in(1,N-1)$.  We define the
  diagonal matrix of eigenvalues $\mymat{\Lambda}$, such that
  $\mymat{\Lambda}_{ij} = \lambda_i\delta_{ij}$
  and write the eigenvalue decomposition in compact form:
  \begin{equation}
    \mymat{\xi} = \mymat{\Psi}\mymat{\Lambda}\mymat{\Psi}^\dagger
  \end{equation}

\item \textbf{KL as a Ranking of Signal-to-Noise}
  It can be shown that KL vectors of a whitened covariance matrix (see
  Section~\ref{Adding_Noise})
  diagonalize both the signal and the noise of the problem, with the
  signal-to-noise ratio proportional to the eigenvalue.  This is
  why KL modes are often called ``Signal-to-noise eigenmodes''.

\item \textbf{KL as an Optimal Low-dimensional Representation:}
  An important consequence of the signal-to-noise properties of KL modes  
  is that the optimal rank-$n$ representation of the data is 
  contained in the KL vectors corresponding to the $n$ largest eigenvalues:
  that is,
  \begin{equation}
    \label{eq_truncation}
    \myvec{\hat\gamma}^{(n)}
    \equiv \sum_{i=1}^{n<N} a_i\myvec{\Psi}_i
  \end{equation}
  minimizes the reconstruction error between $\myvec{\hat\gamma}^{(n)}$ and 
  $\myvec\gamma$ for reconstructions using $n$ orthogonal basis vectors.
  This is the theoretical basis of Principal Component Analysis (sometimes
  called Discrete KL), and leads to a common application of KL 
  decomposition: filtration of noisy signals.  For notational compactness,
  we will define the truncated eigenbasis $\mymat{\Psi}_{(n)}$ and truncated
  vector of coefficients $\myvec{a}_{(n)}$ such that 
  Equation~\ref{eq_truncation} can be written in matrix form:
  $\myvec{\hat\gamma}^{(n)} = \mymat{\Psi}_{(n)}\myvec{a}_{(n)}$.
\end{enumerate}
 
\subsection{KL in the Presence of Noise}
\label{Adding_Noise}
When noise is present in the data, the above properties do not 
necessarily hold.
To satisfy the statistical orthogonality of the KL coefficients $\myvec{a}$ 
and the resulting signal-to-noise properties of the KL eigenmodes, 
it is essential that the noise in the covariance matrix be ``white'': 
that is, $\Noise_\gamma \equiv 
\langle \myvec{n}_\gamma\myvec{n}_\gamma^\dagger \rangle \propto \mymat{I}$.  
This can be accomplished through a judicious
choice of binning, or by rescaling the covariance with a whitening 
transformation.  We take the latter approach here.

Defining the noise covariance matrix $\Noise_\gamma$ as above,
the whitened covariance matrix can be written 
$\myvec{\xi}_W = \Noise_\gamma^{-1/2} \myvec{\xi} \Noise_\gamma^{-1/2}$.  Then 
the whitened KL modes become
$\myvec{\Psi}_W\myvec{\Lambda}_W\myvec{\Psi}_W^\dagger \equiv \myvec{\xi}_W$.
The coefficients $\myvec{a}_W$ are calculated from the noise-weighted signal,
that is
\begin{equation}
  \myvec{a}_W = \myvec{\Psi}_W^\dagger\Noise_\gamma^{-1/2}
  (\myvec{\gamma}+\myvec{n}_\gamma)
\end{equation}
For the whitened KL modes, if signal and noise are uncorrelated, this leads to 
$\langle\myvec{a}_W\myvec{a}_W^\dagger\rangle = \myvec{\Lambda}_W + \myvec{I}$:
that is, the coefficients $\myvec{a}_W$ are statistically orthogonal.
For the remainder of this work, we will drop the subscript ``$_W$'' and assume
all quantities to be those associated with the whitened covariance.



\section{KL for Parameter Estimation}
\label{sec:kl_intro}
KL analysis and the related Principal Component Analysis are well-known
statistical tools which have been applied in a wide variety of astrophysical
situations, from e.g. analysis of the spatial power of galaxy counts
\citep{Vogeley96, Szalay03, Pope04}
to characterization of stellar, galaxy, and QSO spectra
\citep{Connolly95, Connolly99, Yip04a, Yip04b},
to studies of noise properties of weak lensing surveys
\citep{Kilbinger06, Munshi06}, and a host of other situations too numerous
to mention here.  Informally, the power of KL/PCA rests in the fact that 
it allows a highly efficient representation of a set of data, highlighting
the components which are most important in the dataset as a whole.
The discussion of KL analysis below derives largely from \citet{Vogeley96},
reexpressed for application in cosmic shear surveys.

Any $D$-dimensional data point may be represented as a linear combination of 
$D$ orthogonal basis functions.  
For example, the data may be $N$ individual galaxy spectra, each with flux
measurements in $D$ wavelength bins.  Each spectrum can be thought of as a
single point in $D$-dimensional parameter space, where each axis corresponds
to the value within a single wavelength bin.  
Geometrically, there is nothing special about
this choice of axes: one could just as easily rotate and translate the axes
to obtain a different but equivalent representation of the same data.

In the case of of a shear survey, our single data vector is the set of
cosmic shear measurements across the sky.  We will divide the sky into $N$
cells in angular and redshift space, at coordinates
$\myvec{x}_i = (\theta_{x,i}, \theta_{y,i}, z_i)$
These cells may be spatially distinct, or they may overlap.
From the ellipticity of the galaxies within each cell, we
estimate the shear
$\gamma_i \equiv \gamma^o(\myvec{x}_i) = 
\gamma(\myvec{x}_i) + n_\gamma(\myvec{x}_i)$
where $\gamma(\myvec{x}_i)$ is the true underlying shear,
and $n_\gamma(\myvec{x}_i)$ is the measurement noise.
Our data vector is then
$\myvec{\gamma} = [\gamma_1, \gamma_2 \cdots \gamma_N]^T$.

We seek to express our set of measurements $\myvec{\gamma}$
as a linear combination of $N$ (possibly complex) 
orthonormal basis vectors
$\{\myvec{\Psi}_j(\myvec{x}_i, j=1,N)\}$ with complex coefficients
$a_j$:
\begin{equation}
  \label{eq:gamma_decomp}
  \gamma_i = \sum_{j=1}^{N} a_j \Psi_j(\myvec{x}_i)
\end{equation}
For conciseness, we'll create the matrix $\mymat{\Psi}$ whose columns are
the basis vectors $\myvec{\Psi}_j$, so that the above equation can be
compactly written $\myvec\gamma = \mymat\Psi\myvec{a}$.  Orthonormality
of the basis vectors leads to the property
$\mymat\Psi^\dagger\mymat\Psi = \mymat{I}$, where $\mymat{I}$ is the identity
matrix: that is, $\mymat\Psi$ is a unitary matrix with
$\mymat\Psi^{-1} = \mymat\Psi^\dagger$.  Observing this, we can easily compute
the coefficients for a particular data vector:
\begin{equation}
  \myvec{a} = \mymat\Psi^\dagger \myvec\gamma.
\end{equation}
We will be testing the likelihood of a particular set of coefficients
$\myvec{a}$.  
The statistical properties of these coefficients can be written in terms of
the covariance of the observed shear:
\begin{equation}
  \label{eq:a_cov}
  \left\langle \myvec{a}\myvec{a}^\dagger \right\rangle 
  =  \mymat\Psi^\dagger
  \left\langle \myvec\gamma\myvec\gamma^\dagger \right\rangle 
  \mymat\Psi
  \equiv \mymat\Psi^\dagger \myvec{\xi}  \mymat\Psi
\end{equation}
where we have defined the observed shear correlation matrix 
$\myvec{\xi} \equiv \left\langle 
\myvec\gamma\myvec\gamma^\dagger \right\rangle$, and angled braces
$\langle\cdots\rangle$ denote expectation value or ensemble average
of a quantity.

Because we hope to perform a likelihood analysis on the coefficients
$\myvec{a}$, it will be useful in likelihood estimation if they are
statistically orthogonal:
\begin{equation}
  \label{eq:a_cov_2}
  \left\langle \myvec{a}\myvec{a}^\dagger \right\rangle_{ij}
  = \left\langle a_i^2 \right\rangle \delta_{ij}
\end{equation}
Comparing Equations \ref{eq:a_cov} \& \ref{eq:a_cov_2} we see that the desired
basis functions are the solution of the eigenvalue problem
\begin{equation}
  \mymat\xi \myvec\Psi_j = \lambda_j \myvec\Psi_j
\end{equation}
where the eigenvalue $\lambda_j = \left\langle a_i^2 \right\rangle$.
By convention, we'll order the eigenvalue/eigenvector pairs such that
$\lambda_i \ge \lambda_{i+1} \forall i\in(1, N-1)$.
Expansion of the data $\myvec\gamma$ into this basis is the discrete form
of KL analysis.

A KL decomposition has a number of useful properties:
\begin{description}
  \item[Uniqueness] A KL decomposition is a unique representation of the data.
    That is, there only a single set of basis vectors which satisfy the above
    properties (up to degeneracies resulting from identical eigenvalues)
    This can be straightforwardly shown in a proof by contradiction
    \citep[e.g.][]{Vogeley96}.
  \item[Efficiency] A partial KL decomposition provides the
    optimal low-rank approximation of an observed data vector.  That is,
    for $n < N$, the partial reconstruction (cf. Eqn.~\ref{eq:gamma_decomp})
    \begin{equation}
      \myvec\gamma^{(n)} \equiv \sum_{j=1}^n a_j \Psi_j
    \end{equation}
    minimizes the average reconstruction error
    $\epsilon_n \equiv |\myvec\gamma - \myvec\gamma^{(n)}|^2$
    for any orthogonal basis set $\mymat\Psi$.  The proof can be easily
    obtained using Lagrangian multipliers \citep[again, see][]{Vogeley96}.
  \item[Signal-to-noise Optimization] As a consequence of the efficiency
    property, it is clear that for data with white noise\footnote{
    By \textit{white noise} we mean that the noise covariance satisfies
    $\mymat{\mathcal{N}}_{ij} \equiv 
    \langle\myvec{n_\gamma}\myvec{n_\gamma}^\dagger\rangle_{ij} 
    \propto \delta_{ij}$}, KL modes provide the
    maximum possible signal-to-noise ratio per mode.  The noise can be assured
    to be white through a judicious choice of binning, or alternatively
    the data can be artificially whitened (See \S\ref{sec:whitening}).
    If signal and noise are uncorrelated, then the covariance of the observed
    shear can be decomposed as
    \begin{equation}
      \mymat{\xi} = \mymat{\mathcal{S}} + \mymat{\mathcal{N}}
    \end{equation}
    Because the noise covariance $\mymat{\mathcal{N}} \equiv 
    \langle\myvec{n_\gamma}\myvec{n_\gamma}^\dagger\rangle$ is proportional
    to the identity by assumption, Diagonalization of $\mymat{\xi}$ results
    in a simultaneous diagonalization of both the signal $\mymat{\mathcal{S}}$
    and the noise $\mymat{\mathcal{N}}$.  Because of this signal-to-noise
    optimization property, KL modes can be proven to be the optimal basis
    for testing of spatial correlations \citep[see Appendix A of][]{Vogeley96}.
\end{description}

\subsection{Shear Noise Properties}
\label{sec:whitening}
The signal-to-noise properties of shear mentioned above are based on the 
requirement that noise be ``white'', that is, the noise covariance is
$\mymat{\mathcal{N}} \equiv 
\langle\myvec{n_\gamma}\myvec{n_\gamma}^\dagger\rangle
= \sigma^2 \mymat{I}$.  Noise in measured shear is affected mainly by the
intrinsic ellipticity and source density, but can also be prone to systematic
effects which lead to noise correlations between pixels.  When the survey
geometry leads to shear with more complicated noise characteristics, a
whitening transformation can be applied.

Given the measured data $\myvec\gamma$ and noise covariance
$\mymat{\mathcal{N}}$, we can define the whitened shear
\begin{equation}
  \myvec{\gamma}^\prime = \mymat{\mathcal{N}}^{-1/2} \myvec{\gamma}
\end{equation}
With this definition, the shear covariance matrix becomes
\begin{eqnarray}
  \mymat{\xi}^\prime 
  &=& \left\langle \myvec{\gamma}^\prime 
  \myvec{\gamma}^{\prime\dagger}\right\rangle \nonumber\\
  &=& \mymat{\mathcal{N}}^{-1/2}\mymat{\xi}
  \mymat{\mathcal{N}}^{-1/2} \nonumber\\
  &=& \mymat{\mathcal{N}}^{-1/2}\left[
    \mymat{\mathcal{S}} + \mymat{\mathcal{N}}
    \right]\mymat{\mathcal{N}}^{-1/2} \nonumber\\
  &=& \mymat{\mathcal{N}}^{-1/2}\mymat{\mathcal{S}}\mymat{\mathcal{N}}^{-1/2} + \mymat{I}
\end{eqnarray}
We see that the whitened signal is $\mymat{\mathcal{S}}^\prime = 
\mymat{\mathcal{N}}^{-1/2}\mymat{\mathcal{S}}\mymat{\mathcal{N}}^{-1/2}$
and the whitened noise is $\mymat{\mathcal{N}}^\prime = \mymat{I}$, the
identity matrix. So this transformation in fact whitens the data covariance,
so that the noise in each bin is constant and uncorrelated.  Given the
whitened measurement covariance $\mymat{\xi}^\prime$, we can find the KL
decomposition which satisfies the eigenvalue problem
\begin{equation}
  \mymat{\xi}^\prime \myvec{\Psi^\prime}_j = 
  \lambda^\prime_j \myvec{\Psi^\prime}_j
\end{equation}
With KL coefficients given by
\begin{equation}
  \myvec{a}^\prime = \mymat{\Psi}^{\prime\dagger}
  \mymat{\mathcal{N}}^{-1/2}\myvec\gamma
\end{equation}
Note that because $\langle\myvec\gamma\rangle = 0$,
the expectation value of the KL coefficients is
\begin{eqnarray}
  \langle\myvec{a}^\prime\rangle 
  &=& \mymat{\mathcal{N}}^{-1/2}\langle\myvec\gamma\rangle\nonumber\\
  &=& 0
\end{eqnarray}
For the remainder of this work, it will be assumed that we are working with
whitened quantities.  The primes will be dropped for notational simplicity.

\subsection{Constructing the Covariance Matrix}
In many applications, the data covariance matrix can be estimated
empirically, using the fact that
\begin{equation}
  \tilde{\myvec\xi} = \lim_{N\to\infty} \sum_{i=1}^N 
  \myvec{\gamma}_i \myvec{\gamma}_i^\dagger
\end{equation}
Unfortunately, in surveys of cosmic shear, we have only a single sky to
observe, so this approach does not work.  Instead, we can construct the
measurement covariance analytically by assuming a theoretical form of the
underlying matter power spectrum.

The measurement covariance $\mymat{\xi}_{ij}$ between two regions of the
sky $A_i$ and $A_j$ is given by
\begin{eqnarray}
  \label{eq:xi_analytic}
  \myvec{\xi}_{ij} 
  &=& \mymat{\mathcal{S}}_{ij} + \mymat{\mathcal{N}}_{ij} \nonumber\\
  &=& \left[\int_{A_i}d^2x_i\int_{A_j}d^2x_j 
    \xi_+(|\myvec{x_i}-\myvec{x_j}|)\right]
  + \mymat{\mathcal{N}}_{ij}
\end{eqnarray}
where $\xi_+(\theta)$ is the ``+'' shear correlation function. 
$\xi_+(\theta)$ is expressible as an integral over the shear power spectrum
weighted by the zeroth-order Bessel function
\citep[see, e.g.][]{Schneider02}:
\begin{equation}
  \label{eq:xi_plus_def}
  \xi_+(\theta) 
  = \frac{1}{2\pi} \int_0^\infty d\ell\ \ell P_\gamma(\ell) J_0(\ell\theta)
\end{equation}
The angular shear power spectrum $P_\gamma(\ell)$ can be expressed as a
weighted line-of-sight integral over the matter power 
spectrum \citep[see, e.g.][]{Takada04}:
\begin{equation}
  \label{eq:P_gamma}
  P_\gamma(\ell) = \int_0^{\chi_s}d\chi W^2(\chi)\chi^{-2}
  P_\delta\left(k=\frac{\ell}{\chi};z(\chi)\right)
\end{equation}
Here $\chi$ is the comoving distance, $\chi_s$ is the distance to the
source, and $W(\chi)$ is the lensing weight function,
\begin{equation}
  \label{eq:lensing_weight}
  W(\chi) = \frac{3\Omega_{m,0}H_0^2}{2a(\chi)}\frac{\chi}{\bar{n}_g}
  \int_{\chi}^{\chi_s}dz\ n(z) \frac{\chi(z)-\chi}{\chi(z)}
\end{equation}
where $n(z)$ is the empirical redshift distribution of galaxies.
The nonlinear mass fluctuation power spectrum $P_\delta(k, z)$ can be
predicted semianalytically: in this work we use the halo model of
\citet{Smith03}.  With this as an input, we can analytically
construct the measurement covariance matrix $\mymat\xi$ using 
Equations~\ref{eq:xi_analytic}-\ref{eq:lensing_weight}.

\subsection{Cosmological Likelihood Analysis with KL}
From the survey geometry and galaxy ellipticities, we measure the
shear $\myvec\gamma$, estimate the noise covariance
$\mymat{\mathcal{N}}$ (see \S\ref{sec:bootstrap}) and derive
the whitened covariance matrix $\mymat\xi$. 
From $\mymat\xi$ we compute the KL basis $\mymat\Psi$ and $\myvec\lambda$.
Using the KL basis, we compute the coefficients
$\myvec{a} = \mymat{\Psi}^\dagger \mymat{\mathcal{N}}^{-1/2} \myvec\gamma$.
Given these KL coefficients $\myvec{a}$, we use a Bayesian framework to
compute the posterior distribution of our cosmological parameters.

Given observations $D$ and prior information $I$, Bayes' theorem specifies the
posterior probability of a model described by the parameters $\{\theta_i\}$:
\begin{equation}
  \label{eq:bayes}
  P(\{\theta_i\}|DI) = P(\{\theta_i\}|I) \frac{P(D|\{\theta_i\}I)}{P(D|I)}
\end{equation}
The term on the LHS is the \textit{posterior} probability of the set of
model parameters $\{\theta_i\}$, which is the quantity we are interested in.

The first term on the RHS is the \textit{prior}.  It quantifies how our prior
information $I$ affects the probabilities of the model parameters.  The 
prior is where information from other surveys (e.g. WMAP, etc) can be
included. The likelihood function for the observed coefficients $\myvec{a}$
enters into the numerator $P(D|\{\theta_i\}I)$.  The denominator $P(D|I)$
is essentially a normalization constant, set so that the sum of probabilities
over the parameter space equals unity.

For a given model $\{\theta_i\}$, we can predict the expected distribution of model KL
coefficients $\myvec{a}_{\{\theta_i\}} \equiv \mymat{\Psi}^\dagger
\mymat{\mathcal{N}}^{-1/2}\myvec{\gamma}$:
\begin{eqnarray}
  \mymat{C}_{\{\theta_i\}}
  & \equiv & \langle\myvec{a}_{\{\theta_i\}}
  \myvec{a}_{\{\theta_i\}}^\dagger\rangle\nonumber\\
  &=& \mymat{\Psi}^\dagger \mymat{\mathcal{N}}^{-1/2} 
  \mymat{\xi}_{\{\theta_i\}}\mymat{\mathcal{N}}^{-1/2}\mymat{\Psi}
\end{eqnarray}
Using this, the measure of departure from the model $m$ is given by the
quadratic form
\begin{equation}
  \chi^2 = \myvec{a}^\dagger\mymat{C}_{\{\theta_i\}}^{-1}\myvec{a}
\end{equation}
The likelihood is then given by
\begin{equation}
  \label{eq:likelihood}
  \mathcal{L}(\myvec{a}|\{\theta_i\}) = 
  (2\pi)^{n/2} |\det(C_{\{\theta_i\}})|^{-1/2}
  \exp(-\chi^2/2)
\end{equation}
where $n$ is the number of degrees of freedom: that is, the number
of eigenmodes included in the analysis.  The likelihood given by
Equation~\ref{eq:likelihood} enters into Equation~\ref{eq:bayes} when
computing the posterior probability.


